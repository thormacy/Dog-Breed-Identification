{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import mxnet as mx\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/shengwan/Desktop/data\"\n",
    "train_dir = \"train\"\n",
    "test_dir = \"test\"\n",
    "train_gy_dir = \"train_gy\"\n",
    "test_gy_dir = \"test_gy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove original directory\n",
    "train_gy_path = os.path.join(data_dir, train_gy_dir)\n",
    "if os.path.exists(train_gy_path):\n",
    "    shutil.rmtree(train_gy_path)\n",
    "    \n",
    "# make new directory\n",
    "if not os.path.exists(train_gy_path):\n",
    "    os.makedirs(train_gy_path)\n",
    "    \n",
    "# get training data id and labels\n",
    "id_labels = pd.read_csv(os.path.join(data_dir, \"labels.csv\"))\n",
    "\n",
    "# construct sym link between train_dir and train_gy_dir\n",
    "for _, (curr_id, curr_breed) in id_labels.iterrows():\n",
    "    dst_dir = os.path.join(train_gy_path, curr_breed)\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.makedirs(dst_dir)\n",
    "    src_loc = os.path.join(data_dir, train_dir, curr_id+\".jpg\")\n",
    "    dst_loc = os.path.join(dst_dir, curr_id+\".jpg\")\n",
    "    os.symlink(src_loc, dst_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove original directory\n",
    "test_gy_path = os.path.join(data_dir, test_gy_dir)\n",
    "if os.path.exists(test_gy_path):\n",
    "    shutil.rmtree(test_gy_path)\n",
    "    \n",
    "# make new directory\n",
    "if not os.path.exists(test_gy_path):\n",
    "    os.makedirs(test_gy_path)\n",
    "\n",
    "# construct sym link between test_dir and test_gy_dir\n",
    "for roor_dir, sub_dir, sub_files in os.walk(os.path.join(data_dir, test_dir)):\n",
    "    for sub_file in sub_files:\n",
    "        dst_dir = os.path.join(test_gy_path, \"0\")\n",
    "        if not os.path.exists(dst_dir):\n",
    "            os.makedirs(dst_dir)\n",
    "        src_loc = os.path.join(data_dir, test_dir, sub_file)\n",
    "        dst_loc = os.path.join(dst_dir, sub_file)\n",
    "        os.symlink(src_loc, dst_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resize images for model\n",
    "preprocess_list = [\n",
    "    lambda img: img.astype(\"float32\")/255,\n",
    "    mx.image.ForceResizeAug((224, 224)),\n",
    "    mx.image.ColorNormalizeAug(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n",
    "    lambda img: nd.transpose(img,(2,0,1))\n",
    "]\n",
    "\n",
    "def image_preprocess(img):\n",
    "    for f in preprocess_list:\n",
    "        img = f(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "def transform(img, label):\n",
    "    return image_preprocess(img), label\n",
    "\n",
    "def load_data(data_dir, load_batch_size = 32, f_trans=transform):\n",
    "    imgs = mx.gluon.data.vision.ImageFolderDataset(data_dir, transform=transform)\n",
    "    data = mx.gluon.data.DataLoader(imgs, load_batch_size, last_batch=\"keep\")\n",
    "    return data\n",
    "\n",
    "#Extract features\n",
    "def extract_features(net, data, ctx):\n",
    "    rst_X, rst_y = [], []\n",
    "    for X, y in tqdm(data):\n",
    "        Xi = net.features(X.as_in_context(ctx))\n",
    "        rst_X.append(Xi.asnumpy())\n",
    "        rst_y.append(y.asnumpy())\n",
    "    return np.concatenate(rst_X, axis=0), np.concatenate(rst_y, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(\"/Users/shengwan/Desktop/data/train_gy\")\n",
    "test_data = load_data(\"/Users/shengwan/Desktop/data/test_gy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file is not found. Downloading.\n",
      "Downloading /Users/shengwan/.mxnet/models/resnet101_v2-7eb2b3cd.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/resnet101_v2-7eb2b3cd.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [26:55<00:00,  5.05s/it]\n",
      "100%|██████████| 324/324 [27:55<00:00,  5.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# Extract features\n",
    "ctx = mx.cpu()\n",
    "resnet101_v2 = mx.gluon.model_zoo.vision.resnet101_v2(pretrained=True, ctx=ctx)\n",
    "X_train_resnet101_v2, y_train = extract_features(resnet101_v2, train_data, ctx)\n",
    "X_test_resnet101_v2, _ = extract_features(resnet101_v2, test_data, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save features\n",
    "import h5py\n",
    "with h5py.File('/Users/shengwan/Desktop/data/resnet101_v2_pretrained_Xy.h5', 'w') as f:\n",
    "    f['X_train_resnet101_v2'] = X_train_resnet101_v2\n",
    "    f['X_test_resnet101_v2'] = X_test_resnet101_v2\n",
    "    f['y_train'] = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "with h5py.File('/Users/shengwan/Desktop/data/resnet101_v2_pretrained_Xy.h5', 'r') as f:\n",
    "    X_train_resnet101_v2 = np.array(f['X_train_resnet101_v2'])\n",
    "    X_test_resnet101_v2 = np.array(f['X_test_resnet101_v2'])\n",
    "    y_train = np.array(f['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatch\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_resnet101_v2, y_train, test_size=0.2)\n",
    "\n",
    "# dataset\n",
    "dataset_train = mx.gluon.data.ArrayDataset(nd.array(X_train), nd.array(y_train))\n",
    "dataset_val = mx.gluon.data.ArrayDataset(nd.array(X_val), nd.array(y_val))\n",
    "\n",
    "# data itet\n",
    "batch_size = 128\n",
    "data_iter_train = mx.gluon.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "data_iter_val = mx.gluon.data.DataLoader(dataset_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    return nd.mean(nd.argmax(output, axis=1) == labels).asscalar()\n",
    "\n",
    "def evaluate(net, data_iter):\n",
    "    softmax_cross_entropy = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    loss, acc, n = 0., 0., 0.\n",
    "    steps = len(data_iter)\n",
    "    for data, label in data_iter:\n",
    "        data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        acc += accuracy(output, label)\n",
    "        loss += nd.mean(softmax_cross_entropy(output, label)).asscalar()\n",
    "    return loss/steps, acc/steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "ctx = mx.cpu()\n",
    "def get_net(ctx):\n",
    "\n",
    "    net = nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(nn.Dense(256, activation='relu'))\n",
    "        net.add(nn.Dropout(0.5))\n",
    "        net.add(nn.Dense(120))\n",
    "\n",
    "    net.initialize(ctx=ctx)\n",
    "    return net\n",
    "\n",
    "#train\n",
    "def train(net, data_iter_train, data_iter_val, ctx, \n",
    "          epochs=50, lr=0.01, mome=0.9, wd=1e-4, lr_decay=0.5, lr_period=20):\n",
    "\n",
    "    softmax_cross_entropy = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = mx.gluon.Trainer(net.collect_params(),  'sgd', {'learning_rate': lr, 'momentum': mome, \n",
    "                                      'wd': wd})\n",
    "    \n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        steps = len(data_iter_train)\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for X, y in data_iter_train:\n",
    "\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "\n",
    "            with mx.autograd.record():\n",
    "                out = net(X)\n",
    "                loss = softmax_cross_entropy(out, y)\n",
    "\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "\n",
    "\n",
    "            train_acc += accuracy(out, y)\n",
    "\n",
    "        val_loss, val_acc = evaluate(net, data_iter_val)\n",
    "        train_loss_list.append(train_loss/steps)\n",
    "        val_loss_list.append(val_loss)\n",
    "        print(\"Epoch %d. loss: %.4f, acc: %.2f%%, val_loss %.4f, val_acc %.2f%%\" % (\n",
    "            epoch+1, train_loss/steps, train_acc/steps*100, val_loss, val_acc*100))\n",
    "        \n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. loss: 3.9675, acc: 18.50%, val_loss 2.3470, val_acc 65.19%\n",
      "Epoch 2. loss: 1.7085, acc: 61.18%, val_loss 0.8299, val_acc 84.31%\n",
      "Epoch 3. loss: 0.9696, acc: 74.92%, val_loss 0.5641, val_acc 85.97%\n",
      "Epoch 4. loss: 0.7579, acc: 78.77%, val_loss 0.4690, val_acc 87.44%\n",
      "Epoch 5. loss: 0.6426, acc: 81.80%, val_loss 0.4376, val_acc 86.90%\n",
      "Epoch 6. loss: 0.5852, acc: 83.06%, val_loss 0.4054, val_acc 88.08%\n",
      "Epoch 7. loss: 0.5290, acc: 84.15%, val_loss 0.3893, val_acc 87.44%\n",
      "Epoch 8. loss: 0.5052, acc: 84.62%, val_loss 0.3878, val_acc 88.07%\n",
      "Epoch 9. loss: 0.4659, acc: 86.17%, val_loss 0.3661, val_acc 88.27%\n",
      "Epoch 10. loss: 0.4468, acc: 86.40%, val_loss 0.3719, val_acc 88.36%\n",
      "Epoch 11. loss: 0.4345, acc: 86.39%, val_loss 0.3607, val_acc 88.71%\n",
      "Epoch 12. loss: 0.4161, acc: 86.99%, val_loss 0.3577, val_acc 88.81%\n",
      "Epoch 13. loss: 0.4016, acc: 87.53%, val_loss 0.3489, val_acc 88.76%\n",
      "Epoch 14. loss: 0.3887, acc: 87.79%, val_loss 0.3478, val_acc 88.17%\n",
      "Epoch 15. loss: 0.3775, acc: 88.36%, val_loss 0.3482, val_acc 88.46%\n",
      "Epoch 16. loss: 0.3651, acc: 88.67%, val_loss 0.3490, val_acc 88.27%\n",
      "Epoch 17. loss: 0.3497, acc: 89.00%, val_loss 0.3474, val_acc 88.42%\n",
      "Epoch 18. loss: 0.3422, acc: 89.01%, val_loss 0.3508, val_acc 88.32%\n",
      "Epoch 19. loss: 0.3308, acc: 89.62%, val_loss 0.3451, val_acc 88.42%\n",
      "Epoch 20. loss: 0.3247, acc: 89.97%, val_loss 0.3407, val_acc 88.46%\n",
      "Epoch 21. loss: 0.3070, acc: 90.37%, val_loss 0.3343, val_acc 88.61%\n",
      "Epoch 22. loss: 0.3010, acc: 90.97%, val_loss 0.3350, val_acc 88.22%\n",
      "Epoch 23. loss: 0.2902, acc: 90.52%, val_loss 0.3308, val_acc 89.00%\n",
      "Epoch 24. loss: 0.2830, acc: 91.19%, val_loss 0.3331, val_acc 88.95%\n",
      "Epoch 25. loss: 0.2883, acc: 90.77%, val_loss 0.3305, val_acc 88.76%\n",
      "Epoch 26. loss: 0.2845, acc: 91.08%, val_loss 0.3304, val_acc 88.61%\n",
      "Epoch 27. loss: 0.2809, acc: 91.14%, val_loss 0.3342, val_acc 88.86%\n",
      "Epoch 28. loss: 0.2685, acc: 91.71%, val_loss 0.3339, val_acc 88.76%\n",
      "Epoch 29. loss: 0.2742, acc: 91.21%, val_loss 0.3353, val_acc 88.86%\n",
      "Epoch 30. loss: 0.2613, acc: 91.75%, val_loss 0.3306, val_acc 89.05%\n",
      "Epoch 31. loss: 0.2719, acc: 91.18%, val_loss 0.3337, val_acc 89.00%\n",
      "Epoch 32. loss: 0.2590, acc: 91.73%, val_loss 0.3355, val_acc 88.56%\n",
      "Epoch 33. loss: 0.2567, acc: 91.86%, val_loss 0.3317, val_acc 89.05%\n",
      "Epoch 34. loss: 0.2509, acc: 92.44%, val_loss 0.3266, val_acc 88.71%\n",
      "Epoch 35. loss: 0.2491, acc: 92.16%, val_loss 0.3336, val_acc 88.86%\n",
      "Epoch 36. loss: 0.2591, acc: 91.97%, val_loss 0.3331, val_acc 88.81%\n",
      "Epoch 37. loss: 0.2422, acc: 92.29%, val_loss 0.3318, val_acc 88.41%\n",
      "Epoch 38. loss: 0.2484, acc: 92.28%, val_loss 0.3280, val_acc 89.05%\n",
      "Epoch 39. loss: 0.2436, acc: 92.24%, val_loss 0.3335, val_acc 88.81%\n",
      "Epoch 40. loss: 0.2391, acc: 92.65%, val_loss 0.3271, val_acc 88.90%\n",
      "Epoch 41. loss: 0.2316, acc: 92.81%, val_loss 0.3277, val_acc 88.91%\n",
      "Epoch 42. loss: 0.2296, acc: 92.64%, val_loss 0.3260, val_acc 89.25%\n",
      "Epoch 43. loss: 0.2332, acc: 92.95%, val_loss 0.3286, val_acc 88.91%\n",
      "Epoch 44. loss: 0.2283, acc: 92.57%, val_loss 0.3267, val_acc 88.90%\n",
      "Epoch 45. loss: 0.2300, acc: 92.70%, val_loss 0.3266, val_acc 89.00%\n",
      "Epoch 46. loss: 0.2248, acc: 92.99%, val_loss 0.3271, val_acc 89.05%\n",
      "Epoch 47. loss: 0.2286, acc: 92.86%, val_loss 0.3242, val_acc 89.20%\n",
      "Epoch 48. loss: 0.2317, acc: 92.68%, val_loss 0.3271, val_acc 89.00%\n",
      "Epoch 49. loss: 0.2178, acc: 93.30%, val_loss 0.3289, val_acc 89.05%\n",
      "Epoch 50. loss: 0.2198, acc: 93.15%, val_loss 0.3281, val_acc 89.15%\n",
      "Epoch 51. loss: 0.2196, acc: 93.48%, val_loss 0.3264, val_acc 89.20%\n",
      "Epoch 52. loss: 0.2158, acc: 93.26%, val_loss 0.3268, val_acc 88.81%\n",
      "Epoch 53. loss: 0.2142, acc: 93.29%, val_loss 0.3267, val_acc 89.25%\n",
      "Epoch 54. loss: 0.2246, acc: 92.96%, val_loss 0.3276, val_acc 89.10%\n",
      "Epoch 55. loss: 0.2114, acc: 93.53%, val_loss 0.3275, val_acc 89.10%\n",
      "Epoch 56. loss: 0.2098, acc: 93.41%, val_loss 0.3272, val_acc 89.25%\n",
      "Epoch 57. loss: 0.2110, acc: 93.64%, val_loss 0.3269, val_acc 89.00%\n",
      "Epoch 58. loss: 0.2204, acc: 92.99%, val_loss 0.3275, val_acc 88.95%\n",
      "Epoch 59. loss: 0.2070, acc: 93.75%, val_loss 0.3291, val_acc 88.86%\n",
      "Epoch 60. loss: 0.2090, acc: 93.35%, val_loss 0.3307, val_acc 89.10%\n",
      "Epoch 61. loss: 0.2040, acc: 93.57%, val_loss 0.3278, val_acc 89.10%\n",
      "Epoch 62. loss: 0.2055, acc: 93.53%, val_loss 0.3280, val_acc 88.95%\n",
      "Epoch 63. loss: 0.2029, acc: 94.03%, val_loss 0.3292, val_acc 89.20%\n",
      "Epoch 64. loss: 0.2036, acc: 93.86%, val_loss 0.3271, val_acc 89.20%\n",
      "Epoch 65. loss: 0.1995, acc: 94.12%, val_loss 0.3275, val_acc 89.44%\n",
      "Epoch 66. loss: 0.1981, acc: 93.93%, val_loss 0.3276, val_acc 89.05%\n",
      "Epoch 67. loss: 0.1953, acc: 94.21%, val_loss 0.3282, val_acc 89.10%\n",
      "Epoch 68. loss: 0.1958, acc: 94.28%, val_loss 0.3265, val_acc 89.10%\n",
      "Epoch 69. loss: 0.1986, acc: 94.03%, val_loss 0.3275, val_acc 89.05%\n",
      "Epoch 70. loss: 0.1993, acc: 93.86%, val_loss 0.3291, val_acc 89.05%\n",
      "Epoch 71. loss: 0.2005, acc: 93.86%, val_loss 0.3295, val_acc 89.05%\n",
      "Epoch 72. loss: 0.2006, acc: 93.82%, val_loss 0.3274, val_acc 89.15%\n",
      "Epoch 73. loss: 0.1942, acc: 94.25%, val_loss 0.3290, val_acc 89.10%\n",
      "Epoch 74. loss: 0.1952, acc: 93.94%, val_loss 0.3280, val_acc 89.10%\n",
      "Epoch 75. loss: 0.2010, acc: 93.94%, val_loss 0.3284, val_acc 88.95%\n",
      "Epoch 76. loss: 0.1936, acc: 94.16%, val_loss 0.3294, val_acc 88.95%\n",
      "Epoch 77. loss: 0.1908, acc: 94.40%, val_loss 0.3294, val_acc 89.00%\n",
      "Epoch 78. loss: 0.1934, acc: 93.96%, val_loss 0.3285, val_acc 89.15%\n",
      "Epoch 79. loss: 0.1956, acc: 94.04%, val_loss 0.3292, val_acc 89.20%\n",
      "Epoch 80. loss: 0.1961, acc: 94.19%, val_loss 0.3287, val_acc 89.20%\n",
      "Epoch 81. loss: 0.1913, acc: 94.26%, val_loss 0.3279, val_acc 89.20%\n",
      "Epoch 82. loss: 0.1929, acc: 94.18%, val_loss 0.3272, val_acc 89.20%\n",
      "Epoch 83. loss: 0.1915, acc: 94.01%, val_loss 0.3274, val_acc 89.20%\n",
      "Epoch 84. loss: 0.1885, acc: 94.27%, val_loss 0.3280, val_acc 89.05%\n",
      "Epoch 85. loss: 0.1948, acc: 93.95%, val_loss 0.3271, val_acc 89.10%\n",
      "Epoch 86. loss: 0.1917, acc: 94.22%, val_loss 0.3272, val_acc 89.20%\n",
      "Epoch 87. loss: 0.1942, acc: 94.27%, val_loss 0.3261, val_acc 89.29%\n",
      "Epoch 88. loss: 0.1901, acc: 94.31%, val_loss 0.3273, val_acc 89.15%\n",
      "Epoch 89. loss: 0.1944, acc: 94.19%, val_loss 0.3275, val_acc 89.29%\n",
      "Epoch 90. loss: 0.1888, acc: 94.14%, val_loss 0.3281, val_acc 89.15%\n",
      "Epoch 91. loss: 0.1883, acc: 94.25%, val_loss 0.3286, val_acc 89.20%\n",
      "Epoch 92. loss: 0.1947, acc: 94.08%, val_loss 0.3286, val_acc 89.15%\n",
      "Epoch 93. loss: 0.1914, acc: 94.13%, val_loss 0.3283, val_acc 89.10%\n",
      "Epoch 94. loss: 0.1884, acc: 93.95%, val_loss 0.3284, val_acc 89.15%\n",
      "Epoch 95. loss: 0.1911, acc: 94.05%, val_loss 0.3283, val_acc 89.10%\n",
      "Epoch 96. loss: 0.1932, acc: 94.20%, val_loss 0.3285, val_acc 89.10%\n",
      "Epoch 97. loss: 0.1901, acc: 94.22%, val_loss 0.3284, val_acc 89.05%\n",
      "Epoch 98. loss: 0.1938, acc: 94.11%, val_loss 0.3286, val_acc 88.95%\n",
      "Epoch 99. loss: 0.1968, acc: 93.64%, val_loss 0.3286, val_acc 88.90%\n",
      "Epoch 100. loss: 0.1917, acc: 94.07%, val_loss 0.3280, val_acc 88.95%\n",
      "Epoch 101. loss: 0.1874, acc: 94.28%, val_loss 0.3279, val_acc 88.81%\n",
      "Epoch 102. loss: 0.1880, acc: 94.22%, val_loss 0.3281, val_acc 88.90%\n",
      "Epoch 103. loss: 0.1878, acc: 94.40%, val_loss 0.3285, val_acc 88.90%\n",
      "Epoch 104. loss: 0.1860, acc: 94.31%, val_loss 0.3282, val_acc 88.90%\n",
      "Epoch 105. loss: 0.1887, acc: 94.03%, val_loss 0.3284, val_acc 88.90%\n",
      "Epoch 106. loss: 0.1861, acc: 94.38%, val_loss 0.3284, val_acc 89.10%\n",
      "Epoch 107. loss: 0.1835, acc: 94.29%, val_loss 0.3281, val_acc 89.05%\n",
      "Epoch 108. loss: 0.1870, acc: 94.48%, val_loss 0.3281, val_acc 89.25%\n",
      "Epoch 109. loss: 0.1857, acc: 94.35%, val_loss 0.3283, val_acc 89.20%\n",
      "Epoch 110. loss: 0.1854, acc: 94.44%, val_loss 0.3284, val_acc 89.20%\n",
      "Epoch 111. loss: 0.1873, acc: 94.17%, val_loss 0.3283, val_acc 89.10%\n",
      "Epoch 112. loss: 0.1864, acc: 94.36%, val_loss 0.3283, val_acc 89.15%\n",
      "Epoch 113. loss: 0.1862, acc: 94.26%, val_loss 0.3284, val_acc 89.15%\n",
      "Epoch 114. loss: 0.1828, acc: 94.63%, val_loss 0.3283, val_acc 89.10%\n",
      "Epoch 115. loss: 0.1910, acc: 94.14%, val_loss 0.3286, val_acc 89.05%\n",
      "Epoch 116. loss: 0.1860, acc: 94.24%, val_loss 0.3285, val_acc 89.10%\n",
      "Epoch 117. loss: 0.1906, acc: 94.41%, val_loss 0.3285, val_acc 89.00%\n",
      "Epoch 118. loss: 0.1796, acc: 94.55%, val_loss 0.3287, val_acc 88.95%\n",
      "Epoch 119. loss: 0.1803, acc: 94.68%, val_loss 0.3287, val_acc 89.05%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120. loss: 0.1878, acc: 94.03%, val_loss 0.3286, val_acc 89.10%\n",
      "Epoch 121. loss: 0.1905, acc: 94.04%, val_loss 0.3285, val_acc 89.00%\n",
      "Epoch 122. loss: 0.1857, acc: 94.20%, val_loss 0.3286, val_acc 89.10%\n",
      "Epoch 123. loss: 0.1810, acc: 94.60%, val_loss 0.3288, val_acc 89.05%\n",
      "Epoch 124. loss: 0.1811, acc: 94.64%, val_loss 0.3289, val_acc 89.05%\n",
      "Epoch 125. loss: 0.1844, acc: 94.58%, val_loss 0.3287, val_acc 89.00%\n",
      "Epoch 126. loss: 0.1853, acc: 94.50%, val_loss 0.3289, val_acc 89.05%\n",
      "Epoch 127. loss: 0.1874, acc: 94.40%, val_loss 0.3289, val_acc 89.05%\n",
      "Epoch 128. loss: 0.1861, acc: 94.35%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 129. loss: 0.1853, acc: 94.37%, val_loss 0.3291, val_acc 89.05%\n",
      "Epoch 130. loss: 0.1830, acc: 94.51%, val_loss 0.3290, val_acc 89.05%\n",
      "Epoch 131. loss: 0.1846, acc: 94.36%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 132. loss: 0.1821, acc: 94.32%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 133. loss: 0.1843, acc: 94.44%, val_loss 0.3291, val_acc 89.00%\n",
      "Epoch 134. loss: 0.1799, acc: 94.50%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 135. loss: 0.1863, acc: 94.35%, val_loss 0.3291, val_acc 89.05%\n",
      "Epoch 136. loss: 0.1837, acc: 94.32%, val_loss 0.3291, val_acc 89.10%\n",
      "Epoch 137. loss: 0.1854, acc: 94.28%, val_loss 0.3293, val_acc 89.05%\n",
      "Epoch 138. loss: 0.1812, acc: 94.76%, val_loss 0.3293, val_acc 89.05%\n",
      "Epoch 139. loss: 0.1896, acc: 94.26%, val_loss 0.3294, val_acc 89.15%\n",
      "Epoch 140. loss: 0.1923, acc: 94.21%, val_loss 0.3293, val_acc 89.20%\n",
      "Epoch 141. loss: 0.1855, acc: 94.44%, val_loss 0.3293, val_acc 89.20%\n",
      "Epoch 142. loss: 0.1868, acc: 94.24%, val_loss 0.3292, val_acc 89.10%\n",
      "Epoch 143. loss: 0.1837, acc: 94.35%, val_loss 0.3292, val_acc 89.05%\n",
      "Epoch 144. loss: 0.1845, acc: 94.18%, val_loss 0.3292, val_acc 89.05%\n",
      "Epoch 145. loss: 0.1877, acc: 94.27%, val_loss 0.3293, val_acc 89.00%\n",
      "Epoch 146. loss: 0.1848, acc: 94.36%, val_loss 0.3293, val_acc 89.00%\n",
      "Epoch 147. loss: 0.1834, acc: 94.47%, val_loss 0.3294, val_acc 89.00%\n",
      "Epoch 148. loss: 0.1867, acc: 94.60%, val_loss 0.3294, val_acc 89.00%\n",
      "Epoch 149. loss: 0.1828, acc: 94.35%, val_loss 0.3294, val_acc 88.95%\n",
      "Epoch 150. loss: 0.1793, acc: 94.76%, val_loss 0.3292, val_acc 88.95%\n",
      "Epoch 151. loss: 0.1839, acc: 94.66%, val_loss 0.3291, val_acc 89.00%\n",
      "Epoch 152. loss: 0.1765, acc: 94.62%, val_loss 0.3291, val_acc 89.00%\n",
      "Epoch 153. loss: 0.1783, acc: 94.70%, val_loss 0.3291, val_acc 89.00%\n",
      "Epoch 154. loss: 0.1821, acc: 94.21%, val_loss 0.3291, val_acc 88.95%\n",
      "Epoch 155. loss: 0.1833, acc: 94.40%, val_loss 0.3291, val_acc 89.00%\n",
      "Epoch 156. loss: 0.1832, acc: 94.35%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 157. loss: 0.1828, acc: 94.28%, val_loss 0.3289, val_acc 88.95%\n",
      "Epoch 158. loss: 0.1810, acc: 94.56%, val_loss 0.3289, val_acc 89.00%\n",
      "Epoch 159. loss: 0.1877, acc: 94.08%, val_loss 0.3289, val_acc 89.00%\n",
      "Epoch 160. loss: 0.1821, acc: 94.62%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 161. loss: 0.1837, acc: 94.59%, val_loss 0.3289, val_acc 89.05%\n",
      "Epoch 162. loss: 0.1841, acc: 94.24%, val_loss 0.3289, val_acc 89.05%\n",
      "Epoch 163. loss: 0.1829, acc: 94.34%, val_loss 0.3289, val_acc 89.05%\n",
      "Epoch 164. loss: 0.1849, acc: 94.34%, val_loss 0.3290, val_acc 89.05%\n",
      "Epoch 165. loss: 0.1853, acc: 94.24%, val_loss 0.3289, val_acc 89.00%\n",
      "Epoch 166. loss: 0.1850, acc: 94.34%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 167. loss: 0.1784, acc: 94.87%, val_loss 0.3290, val_acc 88.95%\n",
      "Epoch 168. loss: 0.1817, acc: 94.28%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 169. loss: 0.1824, acc: 94.68%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 170. loss: 0.1833, acc: 94.44%, val_loss 0.3290, val_acc 88.95%\n",
      "Epoch 171. loss: 0.1805, acc: 94.65%, val_loss 0.3291, val_acc 89.00%\n",
      "Epoch 172. loss: 0.1887, acc: 94.20%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 173. loss: 0.1863, acc: 94.41%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 174. loss: 0.1795, acc: 94.49%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 175. loss: 0.1833, acc: 94.53%, val_loss 0.3290, val_acc 88.95%\n",
      "Epoch 176. loss: 0.1786, acc: 94.72%, val_loss 0.3290, val_acc 88.95%\n",
      "Epoch 177. loss: 0.1805, acc: 94.42%, val_loss 0.3290, val_acc 88.95%\n",
      "Epoch 178. loss: 0.1822, acc: 94.40%, val_loss 0.3290, val_acc 88.95%\n",
      "Epoch 179. loss: 0.1860, acc: 94.29%, val_loss 0.3290, val_acc 88.95%\n",
      "Epoch 180. loss: 0.1827, acc: 94.59%, val_loss 0.3291, val_acc 89.00%\n",
      "Epoch 181. loss: 0.1801, acc: 94.67%, val_loss 0.3291, val_acc 89.00%\n",
      "Epoch 182. loss: 0.1814, acc: 94.78%, val_loss 0.3291, val_acc 89.00%\n",
      "Epoch 183. loss: 0.1876, acc: 94.18%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 184. loss: 0.1862, acc: 94.41%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 185. loss: 0.1876, acc: 94.32%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 186. loss: 0.1840, acc: 94.46%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 187. loss: 0.1834, acc: 94.66%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 188. loss: 0.1785, acc: 94.49%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 189. loss: 0.1862, acc: 94.44%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 190. loss: 0.1794, acc: 94.81%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 191. loss: 0.1857, acc: 94.44%, val_loss 0.3290, val_acc 88.95%\n",
      "Epoch 192. loss: 0.1861, acc: 94.20%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 193. loss: 0.1827, acc: 94.67%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 194. loss: 0.1768, acc: 94.74%, val_loss 0.3290, val_acc 89.00%\n",
      "Epoch 195. loss: 0.1823, acc: 94.43%, val_loss 0.3289, val_acc 89.00%\n",
      "Epoch 196. loss: 0.1783, acc: 94.71%, val_loss 0.3289, val_acc 89.00%\n",
      "Epoch 197. loss: 0.1774, acc: 94.86%, val_loss 0.3289, val_acc 89.00%\n",
      "Epoch 198. loss: 0.1841, acc: 94.36%, val_loss 0.3289, val_acc 89.00%\n",
      "Epoch 199. loss: 0.1883, acc: 94.30%, val_loss 0.3289, val_acc 89.00%\n",
      "Epoch 200. loss: 0.1746, acc: 94.77%, val_loss 0.3290, val_acc 89.00%\n"
     ]
    }
   ],
   "source": [
    "net = get_net(ctx)\n",
    "train_loss_list, val_loss_list = train(net, data_iter_train, data_iter_val, ctx, epochs=200, lr=0.01, \\\n",
    "      mome=0.9, wd=1e-4, lr_decay=0.5, lr_period=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1.5)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKg5puCulVBzScFdKqTj0/wF12TPcTiklLgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12442ef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list, label=\"train_loss\")\n",
    "plt.plot(val_loss_list, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.ylim([0,1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_softmax = nd.softmax(net(nd.array(X_test_resnet101_v2).as_in_context(ctx)))\n",
    "\n",
    "synsets = mx.gluon.data.vision.ImageFolderDataset(\"/Users/shengwan/Desktop/data/train_gy\").synsets\n",
    "ids = sorted(os.listdir(\"/Users/shengwan/Desktop/data/test_gy/0/\"))\n",
    "ids = [i[:-4] for i in ids]\n",
    "\n",
    "df = pd.DataFrame(out_softmax.asnumpy())\n",
    "df.columns = synsets\n",
    "df[\"id\"] = ids\n",
    "df = df[[\"id\"]+synsets]\n",
    "df.to_csv('/Users/shengwan/Desktop/data/pred-resnet101_v2.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
