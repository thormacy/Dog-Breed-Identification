{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import mxnet as mx\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/shengwan/Desktop/data\"\n",
    "train_dir = \"train\"\n",
    "test_dir = \"test\"\n",
    "train_gy_dir = \"train_gy\"\n",
    "test_gy_dir = \"test_gy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove original directory\n",
    "train_gy_path = os.path.join(data_dir, train_gy_dir)\n",
    "if os.path.exists(train_gy_path):\n",
    "    shutil.rmtree(train_gy_path)\n",
    "    \n",
    "# make new directory\n",
    "if not os.path.exists(train_gy_path):\n",
    "    os.makedirs(train_gy_path)\n",
    "    \n",
    "# get training data id and labels\n",
    "id_labels = pd.read_csv(os.path.join(data_dir, \"labels.csv\"))\n",
    "\n",
    "# construct sym link between train_dir and train_gy_dir\n",
    "for _, (curr_id, curr_breed) in id_labels.iterrows():\n",
    "    dst_dir = os.path.join(train_gy_path, curr_breed)\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.makedirs(dst_dir)\n",
    "    src_loc = os.path.join(data_dir, train_dir, curr_id+\".jpg\")\n",
    "    dst_loc = os.path.join(dst_dir, curr_id+\".jpg\")\n",
    "    os.symlink(src_loc, dst_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove original directory\n",
    "test_gy_path = os.path.join(data_dir, test_gy_dir)\n",
    "if os.path.exists(test_gy_path):\n",
    "    shutil.rmtree(test_gy_path)\n",
    "    \n",
    "# make new directory\n",
    "if not os.path.exists(test_gy_path):\n",
    "    os.makedirs(test_gy_path)\n",
    "\n",
    "# construct sym link between test_dir and test_gy_dir\n",
    "for roor_dir, sub_dir, sub_files in os.walk(os.path.join(data_dir, test_dir)):\n",
    "    for sub_file in sub_files:\n",
    "        dst_dir = os.path.join(test_gy_path, \"0\")\n",
    "        if not os.path.exists(dst_dir):\n",
    "            os.makedirs(dst_dir)\n",
    "        src_loc = os.path.join(data_dir, test_dir, sub_file)\n",
    "        dst_loc = os.path.join(dst_dir, sub_file)\n",
    "        os.symlink(src_loc, dst_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resize images for model\n",
    "preprocess_list = [\n",
    "    lambda img: img.astype(\"float32\")/255,\n",
    "    mx.image.ForceResizeAug((224, 224)),\n",
    "    mx.image.ColorNormalizeAug(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n",
    "    lambda img: nd.transpose(img,(2,0,1))\n",
    "]\n",
    "\n",
    "def image_preprocess(img):\n",
    "    for f in preprocess_list:\n",
    "        img = f(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "def transform(img, label):\n",
    "    return image_preprocess(img), label\n",
    "\n",
    "def load_data(data_dir, load_batch_size = 32, f_trans=transform):\n",
    "    imgs = mx.gluon.data.vision.ImageFolderDataset(data_dir, transform=transform)\n",
    "    data = mx.gluon.data.DataLoader(imgs, load_batch_size, last_batch=\"keep\")\n",
    "    return data\n",
    "\n",
    "#Extract features\n",
    "def extract_features(net, data, ctx):\n",
    "    rst_X, rst_y = [], []\n",
    "    for X, y in tqdm(data):\n",
    "        Xi = net.features(X.as_in_context(ctx))\n",
    "        rst_X.append(Xi.asnumpy())\n",
    "        rst_y.append(y.asnumpy())\n",
    "    return np.concatenate(rst_X, axis=0), np.concatenate(rst_y, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(\"/Users/shengwan/Desktop/data/train_gy\")\n",
    "test_data = load_data(\"/Users/shengwan/Desktop/data/test_gy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file is not found. Downloading.\n",
      "Downloading /Users/shengwan/.mxnet/models/resnet18_v2-8aacf80f.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/resnet18_v2-8aacf80f.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [08:03<00:00,  1.51s/it]\n",
      "100%|██████████| 324/324 [08:09<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# Extract features\n",
    "ctx = mx.cpu()\n",
    "resnet18_v2 = mx.gluon.model_zoo.vision.resnet18_v2(pretrained=True, ctx=ctx)\n",
    "X_train_resnet18_v2, y_train = extract_features(resnet18_v2, train_data, ctx)\n",
    "X_test_resnet18_v2, _ = extract_features(resnet18_v2, test_data, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save features\n",
    "import h5py\n",
    "with h5py.File('/Users/shengwan/Desktop/data/resnet18_v2_pretrained_Xy.h5', 'w') as f:\n",
    "    f['X_train_resnet18_v2'] = X_train_resnet18_v2\n",
    "    f['X_test_resnet18_v2'] = X_test_resnet18_v2\n",
    "    f['y_train'] = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "with h5py.File('/Users/shengwan/Desktop/data/resnet18_v2_pretrained_Xy.h5', 'r') as f:\n",
    "    X_train_resnet18_v2 = np.array(f['X_train_resnet18_v2'])\n",
    "    X_test_resnet18_v2 = np.array(f['X_test_resnet18_v2'])\n",
    "    y_train = np.array(f['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatch\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_resnet18_v2, y_train, test_size=0.2)\n",
    "\n",
    "# dataset\n",
    "dataset_train = mx.gluon.data.ArrayDataset(nd.array(X_train), nd.array(y_train))\n",
    "dataset_val = mx.gluon.data.ArrayDataset(nd.array(X_val), nd.array(y_val))\n",
    "\n",
    "# data itet\n",
    "batch_size = 128\n",
    "data_iter_train = mx.gluon.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "data_iter_val = mx.gluon.data.DataLoader(dataset_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    return nd.mean(nd.argmax(output, axis=1) == labels).asscalar()\n",
    "\n",
    "def evaluate(net, data_iter):\n",
    "    softmax_cross_entropy = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    loss, acc, n = 0., 0., 0.\n",
    "    steps = len(data_iter)\n",
    "    for data, label in data_iter:\n",
    "        data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        acc += accuracy(output, label)\n",
    "        loss += nd.mean(softmax_cross_entropy(output, label)).asscalar()\n",
    "    return loss/steps, acc/steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "ctx = mx.cpu()\n",
    "def get_net(ctx):\n",
    "\n",
    "    net = nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(nn.Dense(256, activation='relu'))\n",
    "        net.add(nn.Dropout(0.5))\n",
    "        net.add(nn.Dense(120))\n",
    "\n",
    "    net.initialize(ctx=ctx)\n",
    "    return net\n",
    "\n",
    "#train\n",
    "def train(net, data_iter_train, data_iter_val, ctx, \n",
    "          epochs=50, lr=0.01, mome=0.9, wd=1e-4, lr_decay=0.5, lr_period=20):\n",
    "\n",
    "    softmax_cross_entropy = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = mx.gluon.Trainer(net.collect_params(),  'sgd', {'learning_rate': lr, 'momentum': mome, \n",
    "                                      'wd': wd})\n",
    "    \n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        steps = len(data_iter_train)\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for X, y in data_iter_train:\n",
    "\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "\n",
    "            with mx.autograd.record():\n",
    "                out = net(X)\n",
    "                loss = softmax_cross_entropy(out, y)\n",
    "\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "\n",
    "\n",
    "            train_acc += accuracy(out, y)\n",
    "\n",
    "        val_loss, val_acc = evaluate(net, data_iter_val)\n",
    "        train_loss_list.append(train_loss/steps)\n",
    "        val_loss_list.append(val_loss)\n",
    "        print(\"Epoch %d. loss: %.4f, acc: %.2f%%, val_loss %.4f, val_acc %.2f%%\" % (\n",
    "            epoch+1, train_loss/steps, train_acc/steps*100, val_loss, val_acc*100))\n",
    "        \n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. loss: 4.1054, acc: 14.03%, val_loss 2.5945, val_acc 57.75%\n",
      "Epoch 2. loss: 1.9601, acc: 51.33%, val_loss 1.0094, val_acc 79.03%\n",
      "Epoch 3. loss: 1.1923, acc: 67.71%, val_loss 0.7444, val_acc 81.28%\n",
      "Epoch 4. loss: 0.9533, acc: 73.08%, val_loss 0.6403, val_acc 82.89%\n",
      "Epoch 5. loss: 0.8177, acc: 75.77%, val_loss 0.5819, val_acc 83.97%\n",
      "Epoch 6. loss: 0.7431, acc: 77.54%, val_loss 0.5613, val_acc 83.43%\n",
      "Epoch 7. loss: 0.6960, acc: 78.42%, val_loss 0.5374, val_acc 84.07%\n",
      "Epoch 8. loss: 0.6470, acc: 80.31%, val_loss 0.5228, val_acc 84.02%\n",
      "Epoch 9. loss: 0.6200, acc: 80.70%, val_loss 0.5233, val_acc 83.68%\n",
      "Epoch 10. loss: 0.5945, acc: 81.78%, val_loss 0.5084, val_acc 84.85%\n",
      "Epoch 11. loss: 0.5661, acc: 82.06%, val_loss 0.5179, val_acc 84.56%\n",
      "Epoch 12. loss: 0.5503, acc: 82.93%, val_loss 0.4974, val_acc 84.31%\n",
      "Epoch 13. loss: 0.5224, acc: 83.84%, val_loss 0.4919, val_acc 84.95%\n",
      "Epoch 14. loss: 0.5068, acc: 84.39%, val_loss 0.4856, val_acc 85.58%\n",
      "Epoch 15. loss: 0.4805, acc: 84.89%, val_loss 0.4856, val_acc 85.14%\n",
      "Epoch 16. loss: 0.4730, acc: 84.92%, val_loss 0.4812, val_acc 85.29%\n",
      "Epoch 17. loss: 0.4545, acc: 85.75%, val_loss 0.4854, val_acc 85.04%\n",
      "Epoch 18. loss: 0.4425, acc: 85.97%, val_loss 0.4845, val_acc 84.99%\n",
      "Epoch 19. loss: 0.4438, acc: 86.21%, val_loss 0.4766, val_acc 85.24%\n",
      "Epoch 20. loss: 0.4163, acc: 86.85%, val_loss 0.4814, val_acc 85.34%\n",
      "Epoch 21. loss: 0.4039, acc: 87.18%, val_loss 0.4686, val_acc 86.17%\n",
      "Epoch 22. loss: 0.3817, acc: 87.81%, val_loss 0.4735, val_acc 85.68%\n",
      "Epoch 23. loss: 0.3764, acc: 87.95%, val_loss 0.4709, val_acc 85.43%\n",
      "Epoch 24. loss: 0.3742, acc: 88.43%, val_loss 0.4705, val_acc 85.87%\n",
      "Epoch 25. loss: 0.3757, acc: 87.92%, val_loss 0.4686, val_acc 85.92%\n",
      "Epoch 26. loss: 0.3615, acc: 88.56%, val_loss 0.4758, val_acc 85.34%\n",
      "Epoch 27. loss: 0.3566, acc: 88.81%, val_loss 0.4748, val_acc 85.34%\n",
      "Epoch 28. loss: 0.3603, acc: 88.50%, val_loss 0.4704, val_acc 85.97%\n",
      "Epoch 29. loss: 0.3465, acc: 89.56%, val_loss 0.4662, val_acc 85.97%\n",
      "Epoch 30. loss: 0.3424, acc: 89.16%, val_loss 0.4714, val_acc 85.97%\n",
      "Epoch 31. loss: 0.3483, acc: 89.12%, val_loss 0.4741, val_acc 85.63%\n",
      "Epoch 32. loss: 0.3445, acc: 88.73%, val_loss 0.4720, val_acc 85.19%\n",
      "Epoch 33. loss: 0.3382, acc: 89.47%, val_loss 0.4717, val_acc 85.63%\n",
      "Epoch 34. loss: 0.3289, acc: 89.74%, val_loss 0.4729, val_acc 85.68%\n",
      "Epoch 35. loss: 0.3211, acc: 90.07%, val_loss 0.4721, val_acc 85.68%\n",
      "Epoch 36. loss: 0.3194, acc: 89.83%, val_loss 0.4724, val_acc 85.29%\n",
      "Epoch 37. loss: 0.3188, acc: 89.90%, val_loss 0.4740, val_acc 85.77%\n",
      "Epoch 38. loss: 0.3126, acc: 90.28%, val_loss 0.4740, val_acc 85.63%\n",
      "Epoch 39. loss: 0.3118, acc: 90.77%, val_loss 0.4764, val_acc 86.02%\n",
      "Epoch 40. loss: 0.3096, acc: 90.16%, val_loss 0.4718, val_acc 85.19%\n",
      "Epoch 41. loss: 0.2998, acc: 90.79%, val_loss 0.4717, val_acc 85.53%\n",
      "Epoch 42. loss: 0.2940, acc: 90.84%, val_loss 0.4702, val_acc 85.43%\n",
      "Epoch 43. loss: 0.2883, acc: 90.94%, val_loss 0.4701, val_acc 85.77%\n",
      "Epoch 44. loss: 0.2949, acc: 90.81%, val_loss 0.4681, val_acc 85.73%\n",
      "Epoch 45. loss: 0.2932, acc: 90.76%, val_loss 0.4709, val_acc 85.53%\n",
      "Epoch 46. loss: 0.2906, acc: 90.62%, val_loss 0.4706, val_acc 85.53%\n",
      "Epoch 47. loss: 0.2819, acc: 91.40%, val_loss 0.4696, val_acc 85.87%\n",
      "Epoch 48. loss: 0.2802, acc: 91.62%, val_loss 0.4722, val_acc 85.68%\n",
      "Epoch 49. loss: 0.2827, acc: 91.25%, val_loss 0.4725, val_acc 85.63%\n",
      "Epoch 50. loss: 0.2796, acc: 91.13%, val_loss 0.4730, val_acc 85.77%\n",
      "Epoch 51. loss: 0.2730, acc: 91.75%, val_loss 0.4713, val_acc 85.48%\n",
      "Epoch 52. loss: 0.2833, acc: 91.16%, val_loss 0.4714, val_acc 85.44%\n",
      "Epoch 53. loss: 0.2781, acc: 91.37%, val_loss 0.4723, val_acc 85.63%\n",
      "Epoch 54. loss: 0.2750, acc: 91.50%, val_loss 0.4739, val_acc 85.82%\n",
      "Epoch 55. loss: 0.2736, acc: 91.49%, val_loss 0.4727, val_acc 85.48%\n",
      "Epoch 56. loss: 0.2687, acc: 91.83%, val_loss 0.4757, val_acc 85.38%\n",
      "Epoch 57. loss: 0.2689, acc: 91.62%, val_loss 0.4760, val_acc 85.48%\n",
      "Epoch 58. loss: 0.2720, acc: 91.46%, val_loss 0.4783, val_acc 85.58%\n",
      "Epoch 59. loss: 0.2662, acc: 91.82%, val_loss 0.4768, val_acc 85.68%\n",
      "Epoch 60. loss: 0.2644, acc: 91.93%, val_loss 0.4748, val_acc 85.77%\n",
      "Epoch 61. loss: 0.2605, acc: 91.75%, val_loss 0.4747, val_acc 85.73%\n",
      "Epoch 62. loss: 0.2564, acc: 92.00%, val_loss 0.4740, val_acc 85.58%\n",
      "Epoch 63. loss: 0.2502, acc: 92.55%, val_loss 0.4726, val_acc 85.68%\n",
      "Epoch 64. loss: 0.2525, acc: 92.10%, val_loss 0.4740, val_acc 85.63%\n",
      "Epoch 65. loss: 0.2530, acc: 92.57%, val_loss 0.4740, val_acc 85.48%\n",
      "Epoch 66. loss: 0.2579, acc: 91.86%, val_loss 0.4730, val_acc 85.63%\n",
      "Epoch 67. loss: 0.2572, acc: 92.22%, val_loss 0.4745, val_acc 85.63%\n",
      "Epoch 68. loss: 0.2494, acc: 92.54%, val_loss 0.4732, val_acc 85.68%\n",
      "Epoch 69. loss: 0.2505, acc: 92.66%, val_loss 0.4754, val_acc 85.58%\n",
      "Epoch 70. loss: 0.2518, acc: 92.39%, val_loss 0.4734, val_acc 85.33%\n",
      "Epoch 71. loss: 0.2408, acc: 92.59%, val_loss 0.4730, val_acc 85.53%\n",
      "Epoch 72. loss: 0.2554, acc: 92.05%, val_loss 0.4740, val_acc 85.73%\n",
      "Epoch 73. loss: 0.2486, acc: 92.56%, val_loss 0.4740, val_acc 85.73%\n",
      "Epoch 74. loss: 0.2498, acc: 92.48%, val_loss 0.4746, val_acc 85.73%\n",
      "Epoch 75. loss: 0.2424, acc: 92.89%, val_loss 0.4746, val_acc 85.87%\n",
      "Epoch 76. loss: 0.2510, acc: 92.35%, val_loss 0.4768, val_acc 85.87%\n",
      "Epoch 77. loss: 0.2421, acc: 92.61%, val_loss 0.4778, val_acc 85.87%\n",
      "Epoch 78. loss: 0.2448, acc: 92.71%, val_loss 0.4773, val_acc 85.63%\n",
      "Epoch 79. loss: 0.2443, acc: 92.62%, val_loss 0.4768, val_acc 85.63%\n",
      "Epoch 80. loss: 0.2545, acc: 92.20%, val_loss 0.4758, val_acc 85.87%\n",
      "Epoch 81. loss: 0.2424, acc: 92.89%, val_loss 0.4765, val_acc 85.77%\n",
      "Epoch 82. loss: 0.2455, acc: 92.54%, val_loss 0.4761, val_acc 85.68%\n",
      "Epoch 83. loss: 0.2344, acc: 93.07%, val_loss 0.4755, val_acc 85.77%\n",
      "Epoch 84. loss: 0.2436, acc: 92.56%, val_loss 0.4766, val_acc 85.73%\n",
      "Epoch 85. loss: 0.2324, acc: 93.28%, val_loss 0.4760, val_acc 85.48%\n",
      "Epoch 86. loss: 0.2476, acc: 92.43%, val_loss 0.4758, val_acc 85.58%\n",
      "Epoch 87. loss: 0.2388, acc: 92.81%, val_loss 0.4752, val_acc 85.53%\n",
      "Epoch 88. loss: 0.2454, acc: 92.87%, val_loss 0.4759, val_acc 85.58%\n",
      "Epoch 89. loss: 0.2444, acc: 92.46%, val_loss 0.4751, val_acc 85.53%\n",
      "Epoch 90. loss: 0.2385, acc: 92.73%, val_loss 0.4763, val_acc 85.58%\n",
      "Epoch 91. loss: 0.2405, acc: 92.75%, val_loss 0.4765, val_acc 85.63%\n",
      "Epoch 92. loss: 0.2437, acc: 92.54%, val_loss 0.4764, val_acc 85.63%\n",
      "Epoch 93. loss: 0.2393, acc: 92.68%, val_loss 0.4760, val_acc 85.73%\n",
      "Epoch 94. loss: 0.2408, acc: 92.95%, val_loss 0.4761, val_acc 85.58%\n",
      "Epoch 95. loss: 0.2419, acc: 92.64%, val_loss 0.4765, val_acc 85.43%\n",
      "Epoch 96. loss: 0.2338, acc: 93.30%, val_loss 0.4771, val_acc 85.48%\n",
      "Epoch 97. loss: 0.2359, acc: 92.70%, val_loss 0.4757, val_acc 85.38%\n",
      "Epoch 98. loss: 0.2310, acc: 93.18%, val_loss 0.4761, val_acc 85.58%\n",
      "Epoch 99. loss: 0.2384, acc: 92.82%, val_loss 0.4766, val_acc 85.63%\n",
      "Epoch 100. loss: 0.2331, acc: 92.93%, val_loss 0.4762, val_acc 85.63%\n",
      "Epoch 101. loss: 0.2371, acc: 92.82%, val_loss 0.4760, val_acc 85.78%\n",
      "Epoch 102. loss: 0.2382, acc: 93.29%, val_loss 0.4763, val_acc 85.78%\n",
      "Epoch 103. loss: 0.2365, acc: 93.02%, val_loss 0.4767, val_acc 85.73%\n",
      "Epoch 104. loss: 0.2325, acc: 92.87%, val_loss 0.4767, val_acc 85.53%\n",
      "Epoch 105. loss: 0.2353, acc: 92.91%, val_loss 0.4769, val_acc 85.58%\n",
      "Epoch 106. loss: 0.2315, acc: 93.25%, val_loss 0.4766, val_acc 85.63%\n",
      "Epoch 107. loss: 0.2321, acc: 93.31%, val_loss 0.4768, val_acc 85.53%\n",
      "Epoch 108. loss: 0.2361, acc: 92.96%, val_loss 0.4764, val_acc 85.48%\n",
      "Epoch 109. loss: 0.2369, acc: 92.77%, val_loss 0.4764, val_acc 85.48%\n",
      "Epoch 110. loss: 0.2301, acc: 93.36%, val_loss 0.4766, val_acc 85.53%\n",
      "Epoch 111. loss: 0.2322, acc: 93.09%, val_loss 0.4766, val_acc 85.58%\n",
      "Epoch 112. loss: 0.2291, acc: 92.96%, val_loss 0.4760, val_acc 85.63%\n",
      "Epoch 113. loss: 0.2329, acc: 93.18%, val_loss 0.4766, val_acc 85.53%\n",
      "Epoch 114. loss: 0.2314, acc: 93.42%, val_loss 0.4768, val_acc 85.68%\n",
      "Epoch 115. loss: 0.2386, acc: 92.74%, val_loss 0.4765, val_acc 85.63%\n",
      "Epoch 116. loss: 0.2331, acc: 93.19%, val_loss 0.4760, val_acc 85.68%\n",
      "Epoch 117. loss: 0.2433, acc: 92.44%, val_loss 0.4764, val_acc 85.63%\n",
      "Epoch 118. loss: 0.2338, acc: 92.80%, val_loss 0.4760, val_acc 85.53%\n",
      "Epoch 119. loss: 0.2333, acc: 93.00%, val_loss 0.4764, val_acc 85.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120. loss: 0.2372, acc: 92.71%, val_loss 0.4769, val_acc 85.73%\n",
      "Epoch 121. loss: 0.2335, acc: 93.24%, val_loss 0.4768, val_acc 85.73%\n",
      "Epoch 122. loss: 0.2366, acc: 92.73%, val_loss 0.4770, val_acc 85.73%\n",
      "Epoch 123. loss: 0.2348, acc: 93.07%, val_loss 0.4767, val_acc 85.77%\n",
      "Epoch 124. loss: 0.2292, acc: 93.37%, val_loss 0.4769, val_acc 85.58%\n",
      "Epoch 125. loss: 0.2290, acc: 93.37%, val_loss 0.4770, val_acc 85.68%\n",
      "Epoch 126. loss: 0.2319, acc: 93.00%, val_loss 0.4769, val_acc 85.63%\n",
      "Epoch 127. loss: 0.2296, acc: 93.20%, val_loss 0.4771, val_acc 85.63%\n",
      "Epoch 128. loss: 0.2266, acc: 93.01%, val_loss 0.4772, val_acc 85.63%\n",
      "Epoch 129. loss: 0.2355, acc: 92.92%, val_loss 0.4768, val_acc 85.58%\n",
      "Epoch 130. loss: 0.2294, acc: 92.83%, val_loss 0.4770, val_acc 85.63%\n",
      "Epoch 131. loss: 0.2325, acc: 93.21%, val_loss 0.4772, val_acc 85.53%\n",
      "Epoch 132. loss: 0.2348, acc: 93.06%, val_loss 0.4770, val_acc 85.63%\n",
      "Epoch 133. loss: 0.2321, acc: 92.96%, val_loss 0.4771, val_acc 85.53%\n",
      "Epoch 134. loss: 0.2345, acc: 92.60%, val_loss 0.4768, val_acc 85.63%\n",
      "Epoch 135. loss: 0.2297, acc: 93.15%, val_loss 0.4769, val_acc 85.68%\n",
      "Epoch 136. loss: 0.2277, acc: 93.28%, val_loss 0.4768, val_acc 85.43%\n",
      "Epoch 137. loss: 0.2320, acc: 93.11%, val_loss 0.4771, val_acc 85.43%\n",
      "Epoch 138. loss: 0.2281, acc: 93.32%, val_loss 0.4767, val_acc 85.58%\n",
      "Epoch 139. loss: 0.2261, acc: 93.59%, val_loss 0.4767, val_acc 85.53%\n",
      "Epoch 140. loss: 0.2291, acc: 93.26%, val_loss 0.4768, val_acc 85.58%\n",
      "Epoch 141. loss: 0.2330, acc: 93.36%, val_loss 0.4769, val_acc 85.53%\n",
      "Epoch 142. loss: 0.2228, acc: 93.65%, val_loss 0.4769, val_acc 85.63%\n",
      "Epoch 143. loss: 0.2250, acc: 93.08%, val_loss 0.4771, val_acc 85.63%\n",
      "Epoch 144. loss: 0.2312, acc: 92.88%, val_loss 0.4772, val_acc 85.63%\n",
      "Epoch 145. loss: 0.2254, acc: 93.25%, val_loss 0.4771, val_acc 85.63%\n",
      "Epoch 146. loss: 0.2300, acc: 92.93%, val_loss 0.4771, val_acc 85.63%\n",
      "Epoch 147. loss: 0.2257, acc: 93.25%, val_loss 0.4770, val_acc 85.68%\n",
      "Epoch 148. loss: 0.2309, acc: 92.99%, val_loss 0.4771, val_acc 85.68%\n",
      "Epoch 149. loss: 0.2290, acc: 93.15%, val_loss 0.4770, val_acc 85.68%\n",
      "Epoch 150. loss: 0.2263, acc: 93.28%, val_loss 0.4771, val_acc 85.63%\n",
      "Epoch 151. loss: 0.2307, acc: 93.14%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 152. loss: 0.2218, acc: 93.60%, val_loss 0.4773, val_acc 85.48%\n",
      "Epoch 153. loss: 0.2289, acc: 93.24%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 154. loss: 0.2275, acc: 93.49%, val_loss 0.4772, val_acc 85.53%\n",
      "Epoch 155. loss: 0.2267, acc: 93.30%, val_loss 0.4771, val_acc 85.48%\n",
      "Epoch 156. loss: 0.2298, acc: 93.35%, val_loss 0.4771, val_acc 85.48%\n",
      "Epoch 157. loss: 0.2316, acc: 93.18%, val_loss 0.4772, val_acc 85.53%\n",
      "Epoch 158. loss: 0.2374, acc: 92.91%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 159. loss: 0.2312, acc: 92.99%, val_loss 0.4772, val_acc 85.53%\n",
      "Epoch 160. loss: 0.2313, acc: 92.87%, val_loss 0.4771, val_acc 85.53%\n",
      "Epoch 161. loss: 0.2323, acc: 92.71%, val_loss 0.4771, val_acc 85.48%\n",
      "Epoch 162. loss: 0.2294, acc: 93.39%, val_loss 0.4771, val_acc 85.48%\n",
      "Epoch 163. loss: 0.2264, acc: 93.42%, val_loss 0.4771, val_acc 85.43%\n",
      "Epoch 164. loss: 0.2323, acc: 93.05%, val_loss 0.4771, val_acc 85.53%\n",
      "Epoch 165. loss: 0.2290, acc: 93.16%, val_loss 0.4772, val_acc 85.58%\n",
      "Epoch 166. loss: 0.2361, acc: 93.12%, val_loss 0.4772, val_acc 85.58%\n",
      "Epoch 167. loss: 0.2287, acc: 93.01%, val_loss 0.4771, val_acc 85.58%\n",
      "Epoch 168. loss: 0.2240, acc: 93.18%, val_loss 0.4770, val_acc 85.53%\n",
      "Epoch 169. loss: 0.2280, acc: 93.34%, val_loss 0.4770, val_acc 85.58%\n",
      "Epoch 170. loss: 0.2311, acc: 92.88%, val_loss 0.4770, val_acc 85.58%\n",
      "Epoch 171. loss: 0.2267, acc: 93.43%, val_loss 0.4770, val_acc 85.58%\n",
      "Epoch 172. loss: 0.2260, acc: 93.54%, val_loss 0.4769, val_acc 85.58%\n",
      "Epoch 173. loss: 0.2283, acc: 93.33%, val_loss 0.4769, val_acc 85.58%\n",
      "Epoch 174. loss: 0.2276, acc: 93.35%, val_loss 0.4770, val_acc 85.58%\n",
      "Epoch 175. loss: 0.2291, acc: 93.41%, val_loss 0.4770, val_acc 85.48%\n",
      "Epoch 176. loss: 0.2291, acc: 93.27%, val_loss 0.4771, val_acc 85.48%\n",
      "Epoch 177. loss: 0.2266, acc: 93.16%, val_loss 0.4771, val_acc 85.48%\n",
      "Epoch 178. loss: 0.2290, acc: 93.19%, val_loss 0.4772, val_acc 85.53%\n",
      "Epoch 179. loss: 0.2275, acc: 93.16%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 180. loss: 0.2296, acc: 93.57%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 181. loss: 0.2254, acc: 93.16%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 182. loss: 0.2337, acc: 92.87%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 183. loss: 0.2257, acc: 93.31%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 184. loss: 0.2307, acc: 93.24%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 185. loss: 0.2293, acc: 92.76%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 186. loss: 0.2321, acc: 93.05%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 187. loss: 0.2273, acc: 93.33%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 188. loss: 0.2265, acc: 93.37%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 189. loss: 0.2309, acc: 93.20%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 190. loss: 0.2330, acc: 93.34%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 191. loss: 0.2283, acc: 93.56%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 192. loss: 0.2306, acc: 93.18%, val_loss 0.4772, val_acc 85.53%\n",
      "Epoch 193. loss: 0.2298, acc: 93.23%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 194. loss: 0.2213, acc: 93.62%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 195. loss: 0.2226, acc: 93.21%, val_loss 0.4772, val_acc 85.58%\n",
      "Epoch 196. loss: 0.2329, acc: 93.03%, val_loss 0.4773, val_acc 85.53%\n",
      "Epoch 197. loss: 0.2217, acc: 93.43%, val_loss 0.4773, val_acc 85.48%\n",
      "Epoch 198. loss: 0.2295, acc: 92.92%, val_loss 0.4773, val_acc 85.48%\n",
      "Epoch 199. loss: 0.2312, acc: 93.00%, val_loss 0.4772, val_acc 85.48%\n",
      "Epoch 200. loss: 0.2324, acc: 92.87%, val_loss 0.4772, val_acc 85.58%\n"
     ]
    }
   ],
   "source": [
    "net = get_net(ctx)\n",
    "train_loss_list, val_loss_list = train(net, data_iter_train, data_iter_val, ctx, epochs=200, lr=0.01, \\\n",
    "      mome=0.9, wd=1e-4, lr_decay=0.5, lr_period=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1.5)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127dd6438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list, label=\"train_loss\")\n",
    "plt.plot(val_loss_list, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.ylim([0,1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_softmax = nd.softmax(net(nd.array(resnet18_v2).as_in_context(ctx)))\n",
    "\n",
    "synsets = mx.gluon.data.vision.ImageFolderDataset(\"/Users/shengwan/Desktop/data/train_gy\").synsets\n",
    "ids = sorted(os.listdir(\"/Users/shengwan/Desktop/data/test_gy/0/\"))\n",
    "ids = [i[:-4] for i in ids]\n",
    "\n",
    "df = pd.DataFrame(out_softmax.asnumpy())\n",
    "df.columns = synsets\n",
    "df[\"id\"] = ids\n",
    "df = df[[\"id\"]+synsets]\n",
    "df.to_csv('/Users/shengwan/Desktop/data/pred-resnet18_v2.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
