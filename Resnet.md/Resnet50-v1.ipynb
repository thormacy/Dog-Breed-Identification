{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import mxnet as mx\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/shengwan/Desktop/data\"\n",
    "train_dir = \"train\"\n",
    "test_dir = \"test\"\n",
    "train_gy_dir = \"train_gy\"\n",
    "test_gy_dir = \"test_gy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove original directory\n",
    "train_gy_path = os.path.join(data_dir, train_gy_dir)\n",
    "if os.path.exists(train_gy_path):\n",
    "    shutil.rmtree(train_gy_path)\n",
    "    \n",
    "# make new directory\n",
    "if not os.path.exists(train_gy_path):\n",
    "    os.makedirs(train_gy_path)\n",
    "    \n",
    "# get training data id and labels\n",
    "id_labels = pd.read_csv(os.path.join(data_dir, \"labels.csv\"))\n",
    "\n",
    "# construct sym link between train_dir and train_gy_dir\n",
    "for _, (curr_id, curr_breed) in id_labels.iterrows():\n",
    "    dst_dir = os.path.join(train_gy_path, curr_breed)\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.makedirs(dst_dir)\n",
    "    src_loc = os.path.join(data_dir, train_dir, curr_id+\".jpg\")\n",
    "    dst_loc = os.path.join(dst_dir, curr_id+\".jpg\")\n",
    "    os.symlink(src_loc, dst_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove original directory\n",
    "test_gy_path = os.path.join(data_dir, test_gy_dir)\n",
    "if os.path.exists(test_gy_path):\n",
    "    shutil.rmtree(test_gy_path)\n",
    "    \n",
    "# make new directory\n",
    "if not os.path.exists(test_gy_path):\n",
    "    os.makedirs(test_gy_path)\n",
    "\n",
    "# construct sym link between test_dir and test_gy_dir\n",
    "for roor_dir, sub_dir, sub_files in os.walk(os.path.join(data_dir, test_dir)):\n",
    "    for sub_file in sub_files:\n",
    "        dst_dir = os.path.join(test_gy_path, \"0\")\n",
    "        if not os.path.exists(dst_dir):\n",
    "            os.makedirs(dst_dir)\n",
    "        src_loc = os.path.join(data_dir, test_dir, sub_file)\n",
    "        dst_loc = os.path.join(dst_dir, sub_file)\n",
    "        os.symlink(src_loc, dst_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resize images for model\n",
    "preprocess_list = [\n",
    "    lambda img: img.astype(\"float32\")/255,\n",
    "    mx.image.ForceResizeAug((224, 224)),\n",
    "    mx.image.ColorNormalizeAug(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n",
    "    lambda img: nd.transpose(img,(2,0,1))\n",
    "]\n",
    "\n",
    "def image_preprocess(img):\n",
    "    for f in preprocess_list:\n",
    "        img = f(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "def transform(img, label):\n",
    "    return image_preprocess(img), label\n",
    "\n",
    "def load_data(data_dir, load_batch_size = 32, f_trans=transform):\n",
    "    imgs = mx.gluon.data.vision.ImageFolderDataset(data_dir, transform=transform)\n",
    "    data = mx.gluon.data.DataLoader(imgs, load_batch_size, last_batch=\"keep\")\n",
    "    return data\n",
    "\n",
    "#Extract features\n",
    "def extract_features(net, data, ctx):\n",
    "    rst_X, rst_y = [], []\n",
    "    for X, y in tqdm(data):\n",
    "        Xi = net.features(X.as_in_context(ctx))\n",
    "        rst_X.append(Xi.asnumpy())\n",
    "        rst_y.append(y.asnumpy())\n",
    "    return np.concatenate(rst_X, axis=0), np.concatenate(rst_y, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(\"/Users/shengwan/Desktop/data/train_gy\")\n",
    "test_data = load_data(\"/Users/shengwan/Desktop/data/test_gy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file is not found. Downloading.\n",
      "Downloading /Users/shengwan/.mxnet/models/resnet50_v1-c940b1a0.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/resnet50_v1-c940b1a0.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [17:43<00:00,  3.32s/it]\n",
      "100%|██████████| 324/324 [18:41<00:00,  3.46s/it]\n"
     ]
    }
   ],
   "source": [
    "# Extract features\n",
    "ctx = mx.cpu()\n",
    "resnet50_v1 = mx.gluon.model_zoo.vision.resnet50_v1(pretrained=True, ctx=ctx)\n",
    "X_train_resnet50_v1, y_train = extract_features(resnet50_v1, train_data, ctx)\n",
    "X_test_resnet50_v1, _ = extract_features(resnet50_v1, test_data, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save features\n",
    "import h5py\n",
    "with h5py.File('/Users/shengwan/Desktop/data/resnet50_v1_pretrained_Xy.h5', 'w') as f:\n",
    "    f['X_train_resnet50_v1'] = X_train_resnet50_v1\n",
    "    f['X_test_resnet50_v1'] = X_test_resnet50_v1\n",
    "    f['y_train'] = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "with h5py.File('/Users/shengwan/Desktop/data/resnet50_v1_pretrained_Xy.h5', 'r') as f:\n",
    "    X_train_resnet50_v1 = np.array(f['X_train_resnet50_v1'])\n",
    "    X_test_resnet50_v1 = np.array(f['X_test_resnet50_v1'])\n",
    "    y_train = np.array(f['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatch\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_resnet50_v1, y_train, test_size=0.2)\n",
    "\n",
    "# dataset\n",
    "dataset_train = mx.gluon.data.ArrayDataset(nd.array(X_train), nd.array(y_train))\n",
    "dataset_val = mx.gluon.data.ArrayDataset(nd.array(X_val), nd.array(y_val))\n",
    "\n",
    "# data itet\n",
    "batch_size = 128\n",
    "data_iter_train = mx.gluon.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "data_iter_val = mx.gluon.data.DataLoader(dataset_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    return nd.mean(nd.argmax(output, axis=1) == labels).asscalar()\n",
    "\n",
    "def evaluate(net, data_iter):\n",
    "    softmax_cross_entropy = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    loss, acc, n = 0., 0., 0.\n",
    "    steps = len(data_iter)\n",
    "    for data, label in data_iter:\n",
    "        data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        acc += accuracy(output, label)\n",
    "        loss += nd.mean(softmax_cross_entropy(output, label)).asscalar()\n",
    "    return loss/steps, acc/steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "ctx = mx.cpu()\n",
    "def get_net(ctx):\n",
    "\n",
    "    net = nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(nn.Dense(256, activation='relu'))\n",
    "        net.add(nn.Dropout(0.5))\n",
    "        net.add(nn.Dense(120))\n",
    "\n",
    "    net.initialize(ctx=ctx)\n",
    "    return net\n",
    "\n",
    "#train\n",
    "def train(net, data_iter_train, data_iter_val, ctx, \n",
    "          epochs=50, lr=0.01, mome=0.9, wd=1e-4, lr_decay=0.5, lr_period=20):\n",
    "\n",
    "    softmax_cross_entropy = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = mx.gluon.Trainer(net.collect_params(),  'sgd', {'learning_rate': lr, 'momentum': mome, \n",
    "                                      'wd': wd})\n",
    "    \n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        steps = len(data_iter_train)\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for X, y in data_iter_train:\n",
    "\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "\n",
    "            with mx.autograd.record():\n",
    "                out = net(X)\n",
    "                loss = softmax_cross_entropy(out, y)\n",
    "\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "\n",
    "\n",
    "            train_acc += accuracy(out, y)\n",
    "\n",
    "        val_loss, val_acc = evaluate(net, data_iter_val)\n",
    "        train_loss_list.append(train_loss/steps)\n",
    "        val_loss_list.append(val_loss)\n",
    "        print(\"Epoch %d. loss: %.4f, acc: %.2f%%, val_loss %.4f, val_acc %.2f%%\" % (\n",
    "            epoch+1, train_loss/steps, train_acc/steps*100, val_loss, val_acc*100))\n",
    "        \n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. loss: 4.0427, acc: 15.19%, val_loss 2.4446, val_acc 63.52%\n",
      "Epoch 2. loss: 1.9036, acc: 53.33%, val_loss 1.0434, val_acc 77.36%\n",
      "Epoch 3. loss: 1.1601, acc: 68.41%, val_loss 0.7525, val_acc 80.78%\n",
      "Epoch 4. loss: 0.9336, acc: 72.61%, val_loss 0.6575, val_acc 82.59%\n",
      "Epoch 5. loss: 0.8215, acc: 75.78%, val_loss 0.5984, val_acc 82.49%\n",
      "Epoch 6. loss: 0.7414, acc: 77.11%, val_loss 0.5690, val_acc 83.81%\n",
      "Epoch 7. loss: 0.6927, acc: 79.21%, val_loss 0.5548, val_acc 83.22%\n",
      "Epoch 8. loss: 0.6379, acc: 80.54%, val_loss 0.5446, val_acc 82.74%\n",
      "Epoch 9. loss: 0.6158, acc: 81.11%, val_loss 0.5334, val_acc 83.67%\n",
      "Epoch 10. loss: 0.5900, acc: 81.81%, val_loss 0.5211, val_acc 84.11%\n",
      "Epoch 11. loss: 0.5518, acc: 82.78%, val_loss 0.5149, val_acc 84.40%\n",
      "Epoch 12. loss: 0.5393, acc: 83.76%, val_loss 0.5247, val_acc 83.47%\n",
      "Epoch 13. loss: 0.5186, acc: 84.03%, val_loss 0.5138, val_acc 83.67%\n",
      "Epoch 14. loss: 0.4972, acc: 84.37%, val_loss 0.5129, val_acc 83.81%\n",
      "Epoch 15. loss: 0.4924, acc: 84.60%, val_loss 0.5069, val_acc 83.77%\n",
      "Epoch 16. loss: 0.4693, acc: 85.18%, val_loss 0.5026, val_acc 83.86%\n",
      "Epoch 17. loss: 0.4607, acc: 85.82%, val_loss 0.5010, val_acc 83.03%\n",
      "Epoch 18. loss: 0.4437, acc: 85.77%, val_loss 0.5025, val_acc 84.40%\n",
      "Epoch 19. loss: 0.4302, acc: 86.34%, val_loss 0.4969, val_acc 84.06%\n",
      "Epoch 20. loss: 0.4215, acc: 86.90%, val_loss 0.5130, val_acc 84.25%\n",
      "Epoch 21. loss: 0.4035, acc: 87.47%, val_loss 0.4986, val_acc 83.82%\n",
      "Epoch 22. loss: 0.3835, acc: 87.88%, val_loss 0.4933, val_acc 84.11%\n",
      "Epoch 23. loss: 0.3769, acc: 88.24%, val_loss 0.4850, val_acc 84.69%\n",
      "Epoch 24. loss: 0.3799, acc: 88.07%, val_loss 0.4920, val_acc 84.06%\n",
      "Epoch 25. loss: 0.3647, acc: 88.52%, val_loss 0.4861, val_acc 84.40%\n",
      "Epoch 26. loss: 0.3567, acc: 88.78%, val_loss 0.4869, val_acc 84.11%\n",
      "Epoch 27. loss: 0.3604, acc: 88.83%, val_loss 0.4832, val_acc 84.11%\n",
      "Epoch 28. loss: 0.3490, acc: 89.38%, val_loss 0.4870, val_acc 84.16%\n",
      "Epoch 29. loss: 0.3488, acc: 88.90%, val_loss 0.4826, val_acc 84.16%\n",
      "Epoch 30. loss: 0.3460, acc: 89.02%, val_loss 0.4883, val_acc 84.79%\n",
      "Epoch 31. loss: 0.3403, acc: 89.49%, val_loss 0.4862, val_acc 84.11%\n",
      "Epoch 32. loss: 0.3373, acc: 89.60%, val_loss 0.4886, val_acc 84.40%\n",
      "Epoch 33. loss: 0.3294, acc: 89.76%, val_loss 0.4882, val_acc 84.35%\n",
      "Epoch 34. loss: 0.3251, acc: 89.80%, val_loss 0.4898, val_acc 84.06%\n",
      "Epoch 35. loss: 0.3237, acc: 89.95%, val_loss 0.4897, val_acc 83.67%\n",
      "Epoch 36. loss: 0.3247, acc: 89.38%, val_loss 0.4941, val_acc 84.16%\n",
      "Epoch 37. loss: 0.3213, acc: 90.01%, val_loss 0.4901, val_acc 84.11%\n",
      "Epoch 38. loss: 0.3107, acc: 90.39%, val_loss 0.4910, val_acc 84.30%\n",
      "Epoch 39. loss: 0.3106, acc: 90.31%, val_loss 0.4813, val_acc 84.55%\n",
      "Epoch 40. loss: 0.3154, acc: 89.93%, val_loss 0.4851, val_acc 84.40%\n",
      "Epoch 41. loss: 0.3041, acc: 91.00%, val_loss 0.4878, val_acc 84.40%\n",
      "Epoch 42. loss: 0.2909, acc: 90.91%, val_loss 0.4875, val_acc 84.55%\n",
      "Epoch 43. loss: 0.2898, acc: 91.11%, val_loss 0.4884, val_acc 84.45%\n",
      "Epoch 44. loss: 0.2865, acc: 90.97%, val_loss 0.4860, val_acc 84.26%\n",
      "Epoch 45. loss: 0.2820, acc: 91.51%, val_loss 0.4881, val_acc 83.96%\n",
      "Epoch 46. loss: 0.2900, acc: 90.85%, val_loss 0.4852, val_acc 84.35%\n",
      "Epoch 47. loss: 0.2782, acc: 91.59%, val_loss 0.4898, val_acc 83.91%\n",
      "Epoch 48. loss: 0.2867, acc: 91.13%, val_loss 0.4879, val_acc 84.30%\n",
      "Epoch 49. loss: 0.2761, acc: 91.40%, val_loss 0.4875, val_acc 83.96%\n",
      "Epoch 50. loss: 0.2846, acc: 90.99%, val_loss 0.4874, val_acc 83.52%\n",
      "Epoch 51. loss: 0.2796, acc: 91.05%, val_loss 0.4872, val_acc 84.45%\n",
      "Epoch 52. loss: 0.2753, acc: 91.54%, val_loss 0.4851, val_acc 84.11%\n",
      "Epoch 53. loss: 0.2732, acc: 92.10%, val_loss 0.4854, val_acc 84.21%\n",
      "Epoch 54. loss: 0.2812, acc: 91.31%, val_loss 0.4898, val_acc 84.35%\n",
      "Epoch 55. loss: 0.2731, acc: 91.46%, val_loss 0.4877, val_acc 84.16%\n",
      "Epoch 56. loss: 0.2677, acc: 91.82%, val_loss 0.4892, val_acc 84.11%\n",
      "Epoch 57. loss: 0.2648, acc: 91.90%, val_loss 0.4907, val_acc 84.11%\n",
      "Epoch 58. loss: 0.2716, acc: 91.69%, val_loss 0.4884, val_acc 84.40%\n",
      "Epoch 59. loss: 0.2655, acc: 91.88%, val_loss 0.4893, val_acc 84.21%\n",
      "Epoch 60. loss: 0.2558, acc: 92.43%, val_loss 0.4902, val_acc 83.91%\n",
      "Epoch 61. loss: 0.2619, acc: 92.04%, val_loss 0.4879, val_acc 84.01%\n",
      "Epoch 62. loss: 0.2621, acc: 92.17%, val_loss 0.4875, val_acc 84.11%\n",
      "Epoch 63. loss: 0.2510, acc: 92.75%, val_loss 0.4876, val_acc 84.16%\n",
      "Epoch 64. loss: 0.2552, acc: 92.46%, val_loss 0.4870, val_acc 84.30%\n",
      "Epoch 65. loss: 0.2636, acc: 91.72%, val_loss 0.4896, val_acc 84.26%\n",
      "Epoch 66. loss: 0.2553, acc: 92.40%, val_loss 0.4887, val_acc 83.77%\n",
      "Epoch 67. loss: 0.2504, acc: 92.78%, val_loss 0.4867, val_acc 84.31%\n",
      "Epoch 68. loss: 0.2580, acc: 92.02%, val_loss 0.4887, val_acc 84.11%\n",
      "Epoch 69. loss: 0.2563, acc: 92.22%, val_loss 0.4895, val_acc 84.26%\n",
      "Epoch 70. loss: 0.2535, acc: 92.48%, val_loss 0.4897, val_acc 84.35%\n",
      "Epoch 71. loss: 0.2536, acc: 92.56%, val_loss 0.4892, val_acc 84.35%\n",
      "Epoch 72. loss: 0.2560, acc: 92.28%, val_loss 0.4882, val_acc 84.26%\n",
      "Epoch 73. loss: 0.2478, acc: 92.29%, val_loss 0.4899, val_acc 84.26%\n",
      "Epoch 74. loss: 0.2522, acc: 92.40%, val_loss 0.4892, val_acc 84.31%\n",
      "Epoch 75. loss: 0.2553, acc: 91.92%, val_loss 0.4908, val_acc 84.40%\n",
      "Epoch 76. loss: 0.2449, acc: 92.78%, val_loss 0.4900, val_acc 84.31%\n",
      "Epoch 77. loss: 0.2556, acc: 92.25%, val_loss 0.4895, val_acc 84.45%\n",
      "Epoch 78. loss: 0.2467, acc: 92.40%, val_loss 0.4880, val_acc 84.16%\n",
      "Epoch 79. loss: 0.2511, acc: 92.13%, val_loss 0.4878, val_acc 84.35%\n",
      "Epoch 80. loss: 0.2475, acc: 92.82%, val_loss 0.4896, val_acc 84.26%\n",
      "Epoch 81. loss: 0.2351, acc: 92.83%, val_loss 0.4890, val_acc 84.45%\n",
      "Epoch 82. loss: 0.2468, acc: 92.79%, val_loss 0.4884, val_acc 84.55%\n",
      "Epoch 83. loss: 0.2456, acc: 92.56%, val_loss 0.4880, val_acc 84.35%\n",
      "Epoch 84. loss: 0.2441, acc: 92.26%, val_loss 0.4880, val_acc 84.40%\n",
      "Epoch 85. loss: 0.2410, acc: 92.56%, val_loss 0.4890, val_acc 84.26%\n",
      "Epoch 86. loss: 0.2479, acc: 92.74%, val_loss 0.4889, val_acc 84.11%\n",
      "Epoch 87. loss: 0.2426, acc: 92.46%, val_loss 0.4887, val_acc 84.31%\n",
      "Epoch 88. loss: 0.2410, acc: 92.89%, val_loss 0.4900, val_acc 84.35%\n",
      "Epoch 89. loss: 0.2433, acc: 92.61%, val_loss 0.4897, val_acc 84.11%\n",
      "Epoch 90. loss: 0.2412, acc: 93.13%, val_loss 0.4895, val_acc 84.01%\n",
      "Epoch 91. loss: 0.2414, acc: 92.62%, val_loss 0.4892, val_acc 84.16%\n",
      "Epoch 92. loss: 0.2445, acc: 92.69%, val_loss 0.4904, val_acc 84.11%\n",
      "Epoch 93. loss: 0.2400, acc: 92.77%, val_loss 0.4908, val_acc 83.96%\n",
      "Epoch 94. loss: 0.2416, acc: 92.87%, val_loss 0.4894, val_acc 84.16%\n",
      "Epoch 95. loss: 0.2394, acc: 93.06%, val_loss 0.4893, val_acc 84.40%\n",
      "Epoch 96. loss: 0.2419, acc: 92.54%, val_loss 0.4894, val_acc 84.36%\n",
      "Epoch 97. loss: 0.2376, acc: 93.33%, val_loss 0.4898, val_acc 84.11%\n",
      "Epoch 98. loss: 0.2374, acc: 92.72%, val_loss 0.4889, val_acc 84.26%\n",
      "Epoch 99. loss: 0.2382, acc: 92.97%, val_loss 0.4881, val_acc 84.40%\n",
      "Epoch 100. loss: 0.2388, acc: 92.87%, val_loss 0.4889, val_acc 84.31%\n",
      "Epoch 101. loss: 0.2316, acc: 93.21%, val_loss 0.4894, val_acc 84.35%\n",
      "Epoch 102. loss: 0.2336, acc: 92.84%, val_loss 0.4900, val_acc 84.26%\n",
      "Epoch 103. loss: 0.2351, acc: 92.85%, val_loss 0.4897, val_acc 84.11%\n",
      "Epoch 104. loss: 0.2391, acc: 92.90%, val_loss 0.4898, val_acc 84.01%\n",
      "Epoch 105. loss: 0.2308, acc: 93.18%, val_loss 0.4899, val_acc 84.31%\n",
      "Epoch 106. loss: 0.2361, acc: 92.85%, val_loss 0.4899, val_acc 84.31%\n",
      "Epoch 107. loss: 0.2336, acc: 92.96%, val_loss 0.4898, val_acc 84.11%\n",
      "Epoch 108. loss: 0.2326, acc: 93.36%, val_loss 0.4894, val_acc 84.21%\n",
      "Epoch 109. loss: 0.2288, acc: 93.07%, val_loss 0.4901, val_acc 84.45%\n",
      "Epoch 110. loss: 0.2394, acc: 92.80%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 111. loss: 0.2334, acc: 93.48%, val_loss 0.4896, val_acc 84.16%\n",
      "Epoch 112. loss: 0.2344, acc: 92.91%, val_loss 0.4890, val_acc 84.35%\n",
      "Epoch 113. loss: 0.2259, acc: 93.44%, val_loss 0.4895, val_acc 84.21%\n",
      "Epoch 114. loss: 0.2311, acc: 93.06%, val_loss 0.4895, val_acc 84.16%\n",
      "Epoch 115. loss: 0.2308, acc: 93.21%, val_loss 0.4895, val_acc 84.16%\n",
      "Epoch 116. loss: 0.2292, acc: 93.42%, val_loss 0.4888, val_acc 84.16%\n",
      "Epoch 117. loss: 0.2276, acc: 93.32%, val_loss 0.4891, val_acc 84.16%\n",
      "Epoch 118. loss: 0.2347, acc: 92.93%, val_loss 0.4888, val_acc 84.26%\n",
      "Epoch 119. loss: 0.2366, acc: 93.13%, val_loss 0.4885, val_acc 84.26%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120. loss: 0.2293, acc: 93.01%, val_loss 0.4887, val_acc 84.31%\n",
      "Epoch 121. loss: 0.2267, acc: 93.30%, val_loss 0.4888, val_acc 84.26%\n",
      "Epoch 122. loss: 0.2333, acc: 93.09%, val_loss 0.4888, val_acc 84.40%\n",
      "Epoch 123. loss: 0.2381, acc: 92.99%, val_loss 0.4889, val_acc 84.36%\n",
      "Epoch 124. loss: 0.2336, acc: 93.10%, val_loss 0.4890, val_acc 84.26%\n",
      "Epoch 125. loss: 0.2363, acc: 93.00%, val_loss 0.4890, val_acc 84.21%\n",
      "Epoch 126. loss: 0.2354, acc: 92.97%, val_loss 0.4891, val_acc 84.16%\n",
      "Epoch 127. loss: 0.2376, acc: 92.78%, val_loss 0.4892, val_acc 84.21%\n",
      "Epoch 128. loss: 0.2327, acc: 92.77%, val_loss 0.4890, val_acc 84.26%\n",
      "Epoch 129. loss: 0.2353, acc: 93.06%, val_loss 0.4888, val_acc 84.26%\n",
      "Epoch 130. loss: 0.2327, acc: 93.13%, val_loss 0.4887, val_acc 84.31%\n",
      "Epoch 131. loss: 0.2304, acc: 93.49%, val_loss 0.4891, val_acc 84.26%\n",
      "Epoch 132. loss: 0.2272, acc: 93.28%, val_loss 0.4893, val_acc 84.31%\n",
      "Epoch 133. loss: 0.2336, acc: 93.04%, val_loss 0.4895, val_acc 84.26%\n",
      "Epoch 134. loss: 0.2288, acc: 93.15%, val_loss 0.4893, val_acc 84.35%\n",
      "Epoch 135. loss: 0.2332, acc: 92.89%, val_loss 0.4894, val_acc 84.35%\n",
      "Epoch 136. loss: 0.2346, acc: 93.20%, val_loss 0.4896, val_acc 84.31%\n",
      "Epoch 137. loss: 0.2280, acc: 93.45%, val_loss 0.4897, val_acc 84.35%\n",
      "Epoch 138. loss: 0.2330, acc: 93.08%, val_loss 0.4899, val_acc 84.45%\n",
      "Epoch 139. loss: 0.2336, acc: 93.29%, val_loss 0.4900, val_acc 84.40%\n",
      "Epoch 140. loss: 0.2385, acc: 92.88%, val_loss 0.4899, val_acc 84.31%\n",
      "Epoch 141. loss: 0.2262, acc: 93.17%, val_loss 0.4897, val_acc 84.40%\n",
      "Epoch 142. loss: 0.2273, acc: 93.27%, val_loss 0.4896, val_acc 84.40%\n",
      "Epoch 143. loss: 0.2281, acc: 93.21%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 144. loss: 0.2298, acc: 93.10%, val_loss 0.4896, val_acc 84.35%\n",
      "Epoch 145. loss: 0.2325, acc: 93.12%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 146. loss: 0.2355, acc: 93.17%, val_loss 0.4897, val_acc 84.31%\n",
      "Epoch 147. loss: 0.2265, acc: 93.26%, val_loss 0.4895, val_acc 84.26%\n",
      "Epoch 148. loss: 0.2338, acc: 93.01%, val_loss 0.4896, val_acc 84.35%\n",
      "Epoch 149. loss: 0.2345, acc: 93.13%, val_loss 0.4896, val_acc 84.40%\n",
      "Epoch 150. loss: 0.2319, acc: 93.04%, val_loss 0.4895, val_acc 84.31%\n",
      "Epoch 151. loss: 0.2217, acc: 93.34%, val_loss 0.4896, val_acc 84.35%\n",
      "Epoch 152. loss: 0.2337, acc: 92.85%, val_loss 0.4896, val_acc 84.35%\n",
      "Epoch 153. loss: 0.2256, acc: 93.11%, val_loss 0.4894, val_acc 84.31%\n",
      "Epoch 154. loss: 0.2302, acc: 93.23%, val_loss 0.4894, val_acc 84.31%\n",
      "Epoch 155. loss: 0.2250, acc: 93.16%, val_loss 0.4894, val_acc 84.35%\n",
      "Epoch 156. loss: 0.2295, acc: 93.20%, val_loss 0.4894, val_acc 84.35%\n",
      "Epoch 157. loss: 0.2279, acc: 93.29%, val_loss 0.4895, val_acc 84.35%\n",
      "Epoch 158. loss: 0.2288, acc: 93.25%, val_loss 0.4895, val_acc 84.26%\n",
      "Epoch 159. loss: 0.2262, acc: 93.34%, val_loss 0.4895, val_acc 84.21%\n",
      "Epoch 160. loss: 0.2264, acc: 93.33%, val_loss 0.4896, val_acc 84.31%\n",
      "Epoch 161. loss: 0.2232, acc: 93.22%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 162. loss: 0.2289, acc: 93.08%, val_loss 0.4897, val_acc 84.26%\n",
      "Epoch 163. loss: 0.2349, acc: 93.12%, val_loss 0.4897, val_acc 84.26%\n",
      "Epoch 164. loss: 0.2251, acc: 93.34%, val_loss 0.4897, val_acc 84.21%\n",
      "Epoch 165. loss: 0.2336, acc: 93.00%, val_loss 0.4897, val_acc 84.21%\n",
      "Epoch 166. loss: 0.2321, acc: 93.02%, val_loss 0.4897, val_acc 84.26%\n",
      "Epoch 167. loss: 0.2231, acc: 93.31%, val_loss 0.4897, val_acc 84.26%\n",
      "Epoch 168. loss: 0.2234, acc: 93.48%, val_loss 0.4899, val_acc 84.26%\n",
      "Epoch 169. loss: 0.2315, acc: 93.10%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 170. loss: 0.2304, acc: 93.26%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 171. loss: 0.2314, acc: 93.05%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 172. loss: 0.2249, acc: 93.48%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 173. loss: 0.2325, acc: 92.90%, val_loss 0.4898, val_acc 84.21%\n",
      "Epoch 174. loss: 0.2326, acc: 93.07%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 175. loss: 0.2266, acc: 93.30%, val_loss 0.4897, val_acc 84.31%\n",
      "Epoch 176. loss: 0.2265, acc: 93.15%, val_loss 0.4897, val_acc 84.31%\n",
      "Epoch 177. loss: 0.2333, acc: 93.06%, val_loss 0.4897, val_acc 84.26%\n",
      "Epoch 178. loss: 0.2271, acc: 93.37%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 179. loss: 0.2303, acc: 93.15%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 180. loss: 0.2333, acc: 92.79%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 181. loss: 0.2329, acc: 93.00%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 182. loss: 0.2321, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 183. loss: 0.2283, acc: 93.20%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 184. loss: 0.2288, acc: 93.23%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 185. loss: 0.2273, acc: 93.27%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 186. loss: 0.2300, acc: 93.27%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 187. loss: 0.2295, acc: 93.21%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 188. loss: 0.2286, acc: 93.40%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 189. loss: 0.2257, acc: 93.40%, val_loss 0.4899, val_acc 84.26%\n",
      "Epoch 190. loss: 0.2290, acc: 93.37%, val_loss 0.4899, val_acc 84.26%\n",
      "Epoch 191. loss: 0.2332, acc: 93.08%, val_loss 0.4898, val_acc 84.21%\n",
      "Epoch 192. loss: 0.2306, acc: 93.26%, val_loss 0.4898, val_acc 84.21%\n",
      "Epoch 193. loss: 0.2284, acc: 93.37%, val_loss 0.4898, val_acc 84.21%\n",
      "Epoch 194. loss: 0.2236, acc: 93.49%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 195. loss: 0.2312, acc: 92.98%, val_loss 0.4898, val_acc 84.21%\n",
      "Epoch 196. loss: 0.2320, acc: 93.06%, val_loss 0.4898, val_acc 84.21%\n",
      "Epoch 197. loss: 0.2307, acc: 93.26%, val_loss 0.4898, val_acc 84.21%\n",
      "Epoch 198. loss: 0.2263, acc: 93.27%, val_loss 0.4898, val_acc 84.21%\n",
      "Epoch 199. loss: 0.2318, acc: 93.45%, val_loss 0.4898, val_acc 84.21%\n",
      "Epoch 200. loss: 0.2270, acc: 93.33%, val_loss 0.4899, val_acc 84.21%\n",
      "Epoch 201. loss: 0.2254, acc: 93.57%, val_loss 0.4899, val_acc 84.21%\n",
      "Epoch 202. loss: 0.2289, acc: 93.25%, val_loss 0.4899, val_acc 84.21%\n",
      "Epoch 203. loss: 0.2326, acc: 92.94%, val_loss 0.4899, val_acc 84.21%\n",
      "Epoch 204. loss: 0.2280, acc: 93.40%, val_loss 0.4899, val_acc 84.21%\n",
      "Epoch 205. loss: 0.2291, acc: 93.40%, val_loss 0.4899, val_acc 84.21%\n",
      "Epoch 206. loss: 0.2263, acc: 93.18%, val_loss 0.4899, val_acc 84.21%\n",
      "Epoch 207. loss: 0.2268, acc: 93.02%, val_loss 0.4899, val_acc 84.21%\n",
      "Epoch 208. loss: 0.2298, acc: 93.40%, val_loss 0.4898, val_acc 84.21%\n",
      "Epoch 209. loss: 0.2278, acc: 93.09%, val_loss 0.4898, val_acc 84.21%\n",
      "Epoch 210. loss: 0.2256, acc: 93.69%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 211. loss: 0.2342, acc: 92.75%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 212. loss: 0.2303, acc: 93.23%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 213. loss: 0.2266, acc: 93.21%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 214. loss: 0.2369, acc: 92.90%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 215. loss: 0.2249, acc: 93.30%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 216. loss: 0.2245, acc: 93.24%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 217. loss: 0.2237, acc: 93.60%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 218. loss: 0.2314, acc: 93.42%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 219. loss: 0.2304, acc: 92.86%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 220. loss: 0.2297, acc: 93.25%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 221. loss: 0.2270, acc: 93.27%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 222. loss: 0.2317, acc: 93.00%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 223. loss: 0.2276, acc: 93.22%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 224. loss: 0.2352, acc: 92.94%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 225. loss: 0.2321, acc: 92.91%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 226. loss: 0.2207, acc: 93.29%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 227. loss: 0.2267, acc: 93.40%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 228. loss: 0.2284, acc: 93.45%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 229. loss: 0.2248, acc: 93.43%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 230. loss: 0.2285, acc: 93.39%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 231. loss: 0.2219, acc: 93.20%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 232. loss: 0.2304, acc: 93.29%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 233. loss: 0.2351, acc: 93.15%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 234. loss: 0.2238, acc: 93.33%, val_loss 0.4898, val_acc 84.26%\n",
      "Epoch 235. loss: 0.2224, acc: 93.51%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 236. loss: 0.2322, acc: 92.95%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 237. loss: 0.2241, acc: 93.30%, val_loss 0.4898, val_acc 84.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238. loss: 0.2267, acc: 93.13%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 239. loss: 0.2276, acc: 93.36%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 240. loss: 0.2311, acc: 93.00%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 241. loss: 0.2285, acc: 93.32%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 242. loss: 0.2285, acc: 93.26%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 243. loss: 0.2268, acc: 93.41%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 244. loss: 0.2282, acc: 93.27%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 245. loss: 0.2271, acc: 93.22%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 246. loss: 0.2288, acc: 93.41%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 247. loss: 0.2310, acc: 93.19%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 248. loss: 0.2311, acc: 93.16%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 249. loss: 0.2324, acc: 93.29%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 250. loss: 0.2194, acc: 93.54%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 251. loss: 0.2314, acc: 93.19%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 252. loss: 0.2311, acc: 93.20%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 253. loss: 0.2367, acc: 92.84%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 254. loss: 0.2263, acc: 93.27%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 255. loss: 0.2338, acc: 92.92%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 256. loss: 0.2264, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 257. loss: 0.2272, acc: 93.25%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 258. loss: 0.2311, acc: 93.13%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 259. loss: 0.2344, acc: 93.10%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 260. loss: 0.2264, acc: 93.40%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 261. loss: 0.2279, acc: 93.30%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 262. loss: 0.2328, acc: 93.52%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 263. loss: 0.2273, acc: 92.99%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 264. loss: 0.2256, acc: 93.57%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 265. loss: 0.2295, acc: 93.21%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 266. loss: 0.2333, acc: 93.17%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 267. loss: 0.2354, acc: 93.25%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 268. loss: 0.2251, acc: 93.42%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 269. loss: 0.2264, acc: 93.25%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 270. loss: 0.2310, acc: 92.86%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 271. loss: 0.2301, acc: 93.35%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 272. loss: 0.2314, acc: 93.02%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 273. loss: 0.2265, acc: 93.41%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 274. loss: 0.2275, acc: 93.48%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 275. loss: 0.2345, acc: 92.98%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 276. loss: 0.2293, acc: 93.39%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 277. loss: 0.2287, acc: 93.36%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 278. loss: 0.2231, acc: 93.16%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 279. loss: 0.2340, acc: 93.04%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 280. loss: 0.2324, acc: 93.22%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 281. loss: 0.2308, acc: 93.35%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 282. loss: 0.2320, acc: 93.30%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 283. loss: 0.2225, acc: 93.20%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 284. loss: 0.2272, acc: 93.38%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 285. loss: 0.2279, acc: 93.25%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 286. loss: 0.2268, acc: 93.24%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 287. loss: 0.2262, acc: 93.16%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 288. loss: 0.2319, acc: 93.13%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 289. loss: 0.2271, acc: 93.32%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 290. loss: 0.2295, acc: 93.03%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 291. loss: 0.2373, acc: 93.04%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 292. loss: 0.2300, acc: 93.06%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 293. loss: 0.2289, acc: 93.04%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 294. loss: 0.2315, acc: 93.28%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 295. loss: 0.2216, acc: 93.54%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 296. loss: 0.2269, acc: 93.45%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 297. loss: 0.2296, acc: 93.12%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 298. loss: 0.2284, acc: 93.11%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 299. loss: 0.2324, acc: 92.91%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 300. loss: 0.2309, acc: 92.90%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 301. loss: 0.2393, acc: 92.82%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 302. loss: 0.2273, acc: 93.34%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 303. loss: 0.2316, acc: 93.04%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 304. loss: 0.2255, acc: 93.23%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 305. loss: 0.2323, acc: 93.06%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 306. loss: 0.2290, acc: 92.88%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 307. loss: 0.2211, acc: 93.40%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 308. loss: 0.2286, acc: 93.35%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 309. loss: 0.2294, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 310. loss: 0.2224, acc: 93.41%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 311. loss: 0.2270, acc: 93.37%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 312. loss: 0.2253, acc: 93.10%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 313. loss: 0.2307, acc: 93.05%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 314. loss: 0.2310, acc: 93.03%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 315. loss: 0.2272, acc: 93.34%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 316. loss: 0.2254, acc: 93.32%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 317. loss: 0.2318, acc: 93.20%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 318. loss: 0.2300, acc: 93.19%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 319. loss: 0.2273, acc: 93.26%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 320. loss: 0.2353, acc: 92.98%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 321. loss: 0.2274, acc: 93.23%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 322. loss: 0.2261, acc: 93.24%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 323. loss: 0.2313, acc: 93.20%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 324. loss: 0.2328, acc: 92.82%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 325. loss: 0.2323, acc: 93.02%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 326. loss: 0.2265, acc: 93.48%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 327. loss: 0.2269, acc: 93.12%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 328. loss: 0.2278, acc: 93.36%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 329. loss: 0.2260, acc: 93.65%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 330. loss: 0.2250, acc: 93.43%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 331. loss: 0.2240, acc: 93.39%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 332. loss: 0.2336, acc: 93.13%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 333. loss: 0.2298, acc: 93.30%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 334. loss: 0.2279, acc: 93.44%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 335. loss: 0.2304, acc: 93.44%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 336. loss: 0.2262, acc: 93.53%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 337. loss: 0.2256, acc: 93.63%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 338. loss: 0.2321, acc: 92.87%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 339. loss: 0.2287, acc: 93.33%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 340. loss: 0.2275, acc: 93.24%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 341. loss: 0.2262, acc: 93.30%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 342. loss: 0.2254, acc: 93.25%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 343. loss: 0.2311, acc: 92.72%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 344. loss: 0.2295, acc: 93.04%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 345. loss: 0.2243, acc: 93.64%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 346. loss: 0.2334, acc: 92.77%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 347. loss: 0.2255, acc: 93.39%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 348. loss: 0.2264, acc: 93.48%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 349. loss: 0.2222, acc: 93.31%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 350. loss: 0.2275, acc: 93.06%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 351. loss: 0.2275, acc: 93.52%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 352. loss: 0.2257, acc: 93.36%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 353. loss: 0.2359, acc: 92.80%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 354. loss: 0.2330, acc: 93.02%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 355. loss: 0.2269, acc: 93.53%, val_loss 0.4898, val_acc 84.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 356. loss: 0.2304, acc: 93.27%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 357. loss: 0.2265, acc: 93.20%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 358. loss: 0.2272, acc: 93.47%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 359. loss: 0.2319, acc: 93.35%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 360. loss: 0.2258, acc: 93.27%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 361. loss: 0.2277, acc: 93.45%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 362. loss: 0.2368, acc: 92.70%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 363. loss: 0.2243, acc: 93.64%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 364. loss: 0.2299, acc: 93.17%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 365. loss: 0.2252, acc: 93.62%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 366. loss: 0.2309, acc: 92.84%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 367. loss: 0.2298, acc: 93.53%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 368. loss: 0.2214, acc: 93.62%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 369. loss: 0.2239, acc: 93.44%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 370. loss: 0.2344, acc: 93.06%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 371. loss: 0.2274, acc: 93.20%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 372. loss: 0.2297, acc: 93.08%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 373. loss: 0.2271, acc: 93.08%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 374. loss: 0.2308, acc: 93.19%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 375. loss: 0.2232, acc: 93.73%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 376. loss: 0.2253, acc: 93.51%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 377. loss: 0.2338, acc: 93.12%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 378. loss: 0.2340, acc: 93.06%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 379. loss: 0.2293, acc: 93.04%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 380. loss: 0.2254, acc: 93.23%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 381. loss: 0.2340, acc: 92.88%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 382. loss: 0.2292, acc: 93.37%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 383. loss: 0.2304, acc: 93.15%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 384. loss: 0.2319, acc: 92.93%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 385. loss: 0.2330, acc: 92.80%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 386. loss: 0.2339, acc: 92.78%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 387. loss: 0.2312, acc: 93.17%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 388. loss: 0.2295, acc: 93.20%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 389. loss: 0.2297, acc: 93.31%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 390. loss: 0.2291, acc: 93.11%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 391. loss: 0.2292, acc: 93.03%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 392. loss: 0.2356, acc: 93.22%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 393. loss: 0.2310, acc: 92.89%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 394. loss: 0.2306, acc: 93.25%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 395. loss: 0.2260, acc: 93.16%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 396. loss: 0.2276, acc: 93.58%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 397. loss: 0.2292, acc: 93.23%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 398. loss: 0.2294, acc: 93.07%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 399. loss: 0.2277, acc: 93.28%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 400. loss: 0.2295, acc: 93.38%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 401. loss: 0.2267, acc: 93.46%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 402. loss: 0.2223, acc: 93.54%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 403. loss: 0.2294, acc: 93.15%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 404. loss: 0.2291, acc: 92.95%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 405. loss: 0.2289, acc: 93.19%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 406. loss: 0.2278, acc: 93.70%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 407. loss: 0.2267, acc: 93.16%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 408. loss: 0.2327, acc: 93.17%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 409. loss: 0.2224, acc: 93.40%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 410. loss: 0.2293, acc: 92.95%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 411. loss: 0.2305, acc: 93.16%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 412. loss: 0.2298, acc: 93.13%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 413. loss: 0.2332, acc: 92.87%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 414. loss: 0.2319, acc: 93.11%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 415. loss: 0.2196, acc: 93.44%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 416. loss: 0.2211, acc: 93.62%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 417. loss: 0.2257, acc: 93.31%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 418. loss: 0.2281, acc: 93.32%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 419. loss: 0.2297, acc: 93.37%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 420. loss: 0.2296, acc: 93.38%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 421. loss: 0.2326, acc: 93.32%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 422. loss: 0.2298, acc: 92.78%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 423. loss: 0.2256, acc: 93.38%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 424. loss: 0.2282, acc: 93.31%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 425. loss: 0.2291, acc: 93.21%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 426. loss: 0.2329, acc: 92.70%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 427. loss: 0.2343, acc: 93.35%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 428. loss: 0.2292, acc: 93.24%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 429. loss: 0.2308, acc: 93.20%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 430. loss: 0.2265, acc: 93.23%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 431. loss: 0.2251, acc: 93.41%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 432. loss: 0.2315, acc: 92.87%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 433. loss: 0.2314, acc: 93.23%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 434. loss: 0.2356, acc: 93.12%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 435. loss: 0.2257, acc: 93.52%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 436. loss: 0.2192, acc: 93.79%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 437. loss: 0.2239, acc: 93.37%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 438. loss: 0.2276, acc: 93.54%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 439. loss: 0.2288, acc: 93.16%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 440. loss: 0.2272, acc: 93.10%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 441. loss: 0.2333, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 442. loss: 0.2330, acc: 93.18%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 443. loss: 0.2353, acc: 92.93%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 444. loss: 0.2305, acc: 93.11%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 445. loss: 0.2329, acc: 93.13%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 446. loss: 0.2308, acc: 93.05%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 447. loss: 0.2234, acc: 93.16%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 448. loss: 0.2325, acc: 93.08%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 449. loss: 0.2314, acc: 93.06%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 450. loss: 0.2260, acc: 93.45%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 451. loss: 0.2274, acc: 93.25%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 452. loss: 0.2304, acc: 92.84%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 453. loss: 0.2290, acc: 92.89%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 454. loss: 0.2283, acc: 93.48%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 455. loss: 0.2265, acc: 93.50%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 456. loss: 0.2307, acc: 93.16%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 457. loss: 0.2282, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 458. loss: 0.2383, acc: 92.78%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 459. loss: 0.2334, acc: 93.25%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 460. loss: 0.2276, acc: 93.39%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 461. loss: 0.2318, acc: 93.18%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 462. loss: 0.2349, acc: 93.17%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 463. loss: 0.2258, acc: 93.43%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 464. loss: 0.2241, acc: 93.64%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 465. loss: 0.2192, acc: 93.62%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 466. loss: 0.2282, acc: 93.42%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 467. loss: 0.2278, acc: 93.22%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 468. loss: 0.2312, acc: 92.92%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 469. loss: 0.2299, acc: 93.11%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 470. loss: 0.2278, acc: 93.26%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 471. loss: 0.2277, acc: 93.47%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 472. loss: 0.2265, acc: 93.28%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 473. loss: 0.2292, acc: 93.25%, val_loss 0.4898, val_acc 84.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 474. loss: 0.2188, acc: 93.41%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 475. loss: 0.2328, acc: 92.98%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 476. loss: 0.2232, acc: 93.38%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 477. loss: 0.2309, acc: 92.81%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 478. loss: 0.2308, acc: 92.98%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 479. loss: 0.2284, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 480. loss: 0.2352, acc: 92.90%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 481. loss: 0.2271, acc: 93.51%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 482. loss: 0.2321, acc: 93.17%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 483. loss: 0.2311, acc: 93.22%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 484. loss: 0.2376, acc: 92.77%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 485. loss: 0.2268, acc: 93.16%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 486. loss: 0.2292, acc: 93.02%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 487. loss: 0.2256, acc: 93.70%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 488. loss: 0.2292, acc: 93.00%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 489. loss: 0.2269, acc: 93.10%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 490. loss: 0.2292, acc: 92.92%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 491. loss: 0.2273, acc: 93.06%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 492. loss: 0.2280, acc: 93.02%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 493. loss: 0.2284, acc: 93.62%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 494. loss: 0.2322, acc: 92.93%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 495. loss: 0.2319, acc: 92.88%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 496. loss: 0.2321, acc: 92.96%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 497. loss: 0.2263, acc: 93.37%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 498. loss: 0.2339, acc: 93.02%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 499. loss: 0.2271, acc: 93.59%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 500. loss: 0.2211, acc: 93.53%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 501. loss: 0.2260, acc: 93.25%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 502. loss: 0.2304, acc: 93.23%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 503. loss: 0.2208, acc: 93.65%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 504. loss: 0.2339, acc: 93.02%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 505. loss: 0.2228, acc: 93.75%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 506. loss: 0.2405, acc: 92.54%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 507. loss: 0.2351, acc: 93.11%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 508. loss: 0.2239, acc: 93.34%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 509. loss: 0.2338, acc: 92.99%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 510. loss: 0.2368, acc: 92.81%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 511. loss: 0.2296, acc: 92.77%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 512. loss: 0.2274, acc: 93.25%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 513. loss: 0.2324, acc: 93.12%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 514. loss: 0.2258, acc: 93.32%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 515. loss: 0.2237, acc: 93.44%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 516. loss: 0.2309, acc: 93.35%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 517. loss: 0.2307, acc: 92.90%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 518. loss: 0.2317, acc: 92.69%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 519. loss: 0.2267, acc: 93.31%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 520. loss: 0.2280, acc: 93.15%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 521. loss: 0.2259, acc: 93.54%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 522. loss: 0.2297, acc: 93.02%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 523. loss: 0.2261, acc: 93.34%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 524. loss: 0.2255, acc: 93.33%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 525. loss: 0.2338, acc: 93.07%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 526. loss: 0.2293, acc: 93.12%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 527. loss: 0.2303, acc: 93.25%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 528. loss: 0.2260, acc: 93.46%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 529. loss: 0.2295, acc: 93.33%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 530. loss: 0.2240, acc: 93.46%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 531. loss: 0.2254, acc: 93.42%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 532. loss: 0.2307, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 533. loss: 0.2230, acc: 93.68%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 534. loss: 0.2265, acc: 93.58%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 535. loss: 0.2290, acc: 93.02%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 536. loss: 0.2301, acc: 93.40%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 537. loss: 0.2273, acc: 93.34%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 538. loss: 0.2295, acc: 93.06%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 539. loss: 0.2315, acc: 92.81%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 540. loss: 0.2305, acc: 93.30%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 541. loss: 0.2273, acc: 93.07%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 542. loss: 0.2238, acc: 93.75%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 543. loss: 0.2352, acc: 93.03%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 544. loss: 0.2274, acc: 93.58%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 545. loss: 0.2260, acc: 93.15%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 546. loss: 0.2226, acc: 93.19%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 547. loss: 0.2284, acc: 93.29%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 548. loss: 0.2294, acc: 93.19%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 549. loss: 0.2272, acc: 93.68%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 550. loss: 0.2238, acc: 93.32%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 551. loss: 0.2323, acc: 92.99%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 552. loss: 0.2298, acc: 93.03%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 553. loss: 0.2267, acc: 93.36%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 554. loss: 0.2264, acc: 93.33%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 555. loss: 0.2304, acc: 93.24%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 556. loss: 0.2329, acc: 93.03%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 557. loss: 0.2338, acc: 92.98%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 558. loss: 0.2270, acc: 93.49%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 559. loss: 0.2311, acc: 93.36%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 560. loss: 0.2274, acc: 93.34%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 561. loss: 0.2293, acc: 93.21%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 562. loss: 0.2343, acc: 93.13%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 563. loss: 0.2257, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 564. loss: 0.2279, acc: 93.35%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 565. loss: 0.2299, acc: 93.26%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 566. loss: 0.2269, acc: 93.34%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 567. loss: 0.2275, acc: 93.04%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 568. loss: 0.2304, acc: 93.17%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 569. loss: 0.2301, acc: 92.99%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 570. loss: 0.2296, acc: 93.21%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 571. loss: 0.2326, acc: 92.93%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 572. loss: 0.2241, acc: 93.46%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 573. loss: 0.2295, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 574. loss: 0.2313, acc: 92.99%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 575. loss: 0.2267, acc: 93.47%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 576. loss: 0.2320, acc: 92.91%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 577. loss: 0.2268, acc: 93.28%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 578. loss: 0.2319, acc: 93.43%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 579. loss: 0.2321, acc: 93.24%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 580. loss: 0.2239, acc: 93.23%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 581. loss: 0.2290, acc: 93.28%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 582. loss: 0.2327, acc: 92.98%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 583. loss: 0.2336, acc: 92.77%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 584. loss: 0.2320, acc: 92.90%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 585. loss: 0.2339, acc: 92.90%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 586. loss: 0.2263, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 587. loss: 0.2249, acc: 93.59%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 588. loss: 0.2351, acc: 92.61%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 589. loss: 0.2286, acc: 93.15%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 590. loss: 0.2293, acc: 93.18%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 591. loss: 0.2306, acc: 93.05%, val_loss 0.4898, val_acc 84.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 592. loss: 0.2345, acc: 92.89%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 593. loss: 0.2306, acc: 93.07%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 594. loss: 0.2258, acc: 93.23%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 595. loss: 0.2327, acc: 93.15%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 596. loss: 0.2358, acc: 92.79%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 597. loss: 0.2298, acc: 93.18%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 598. loss: 0.2289, acc: 92.91%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 599. loss: 0.2290, acc: 93.20%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 600. loss: 0.2280, acc: 93.32%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 601. loss: 0.2308, acc: 93.28%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 602. loss: 0.2277, acc: 93.13%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 603. loss: 0.2310, acc: 93.14%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 604. loss: 0.2308, acc: 93.04%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 605. loss: 0.2267, acc: 93.23%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 606. loss: 0.2299, acc: 93.08%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 607. loss: 0.2365, acc: 92.93%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 608. loss: 0.2293, acc: 93.21%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 609. loss: 0.2326, acc: 93.29%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 610. loss: 0.2325, acc: 92.95%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 611. loss: 0.2316, acc: 92.94%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 612. loss: 0.2277, acc: 93.01%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 613. loss: 0.2259, acc: 93.47%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 614. loss: 0.2329, acc: 92.99%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 615. loss: 0.2270, acc: 93.39%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 616. loss: 0.2252, acc: 93.03%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 617. loss: 0.2280, acc: 93.22%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 618. loss: 0.2303, acc: 93.32%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 619. loss: 0.2259, acc: 93.48%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 620. loss: 0.2283, acc: 93.04%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 621. loss: 0.2308, acc: 92.90%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 622. loss: 0.2283, acc: 93.39%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 623. loss: 0.2266, acc: 93.55%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 624. loss: 0.2250, acc: 93.31%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 625. loss: 0.2351, acc: 92.73%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 626. loss: 0.2217, acc: 93.76%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 627. loss: 0.2276, acc: 93.27%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 628. loss: 0.2289, acc: 92.98%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 629. loss: 0.2267, acc: 93.24%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 630. loss: 0.2297, acc: 93.20%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 631. loss: 0.2302, acc: 93.07%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 632. loss: 0.2259, acc: 93.42%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 633. loss: 0.2246, acc: 93.43%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 634. loss: 0.2317, acc: 93.37%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 635. loss: 0.2297, acc: 93.29%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 636. loss: 0.2345, acc: 93.03%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 637. loss: 0.2331, acc: 93.31%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 638. loss: 0.2209, acc: 93.69%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 639. loss: 0.2339, acc: 93.16%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 640. loss: 0.2308, acc: 93.18%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 641. loss: 0.2251, acc: 93.76%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 642. loss: 0.2282, acc: 93.39%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 643. loss: 0.2290, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 644. loss: 0.2284, acc: 93.36%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 645. loss: 0.2292, acc: 93.03%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 646. loss: 0.2284, acc: 93.06%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 647. loss: 0.2369, acc: 92.78%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 648. loss: 0.2305, acc: 93.19%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 649. loss: 0.2261, acc: 93.23%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 650. loss: 0.2297, acc: 93.12%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 651. loss: 0.2269, acc: 93.30%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 652. loss: 0.2334, acc: 92.99%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 653. loss: 0.2281, acc: 93.36%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 654. loss: 0.2244, acc: 93.46%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 655. loss: 0.2263, acc: 93.28%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 656. loss: 0.2258, acc: 93.39%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 657. loss: 0.2337, acc: 93.11%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 658. loss: 0.2353, acc: 92.70%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 659. loss: 0.2305, acc: 93.14%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 660. loss: 0.2332, acc: 92.89%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 661. loss: 0.2362, acc: 92.54%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 662. loss: 0.2258, acc: 93.47%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 663. loss: 0.2265, acc: 93.34%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 664. loss: 0.2256, acc: 93.52%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 665. loss: 0.2320, acc: 92.89%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 666. loss: 0.2285, acc: 93.12%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 667. loss: 0.2305, acc: 93.18%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 668. loss: 0.2278, acc: 93.11%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 669. loss: 0.2319, acc: 92.84%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 670. loss: 0.2282, acc: 93.33%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 671. loss: 0.2316, acc: 93.05%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 672. loss: 0.2319, acc: 93.14%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 673. loss: 0.2273, acc: 93.19%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 674. loss: 0.2310, acc: 92.89%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 675. loss: 0.2366, acc: 93.05%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 676. loss: 0.2244, acc: 93.78%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 677. loss: 0.2336, acc: 93.02%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 678. loss: 0.2196, acc: 93.43%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 679. loss: 0.2325, acc: 92.94%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 680. loss: 0.2326, acc: 92.96%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 681. loss: 0.2314, acc: 92.80%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 682. loss: 0.2307, acc: 92.96%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 683. loss: 0.2313, acc: 93.07%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 684. loss: 0.2292, acc: 93.36%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 685. loss: 0.2229, acc: 93.67%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 686. loss: 0.2362, acc: 92.86%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 687. loss: 0.2290, acc: 93.38%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 688. loss: 0.2229, acc: 93.51%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 689. loss: 0.2298, acc: 93.16%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 690. loss: 0.2296, acc: 93.11%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 691. loss: 0.2300, acc: 93.30%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 692. loss: 0.2302, acc: 93.01%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 693. loss: 0.2274, acc: 93.12%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 694. loss: 0.2303, acc: 93.49%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 695. loss: 0.2288, acc: 93.20%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 696. loss: 0.2258, acc: 93.34%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 697. loss: 0.2246, acc: 93.43%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 698. loss: 0.2301, acc: 93.26%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 699. loss: 0.2307, acc: 93.01%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 700. loss: 0.2266, acc: 93.68%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 701. loss: 0.2302, acc: 92.96%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 702. loss: 0.2250, acc: 93.31%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 703. loss: 0.2371, acc: 92.97%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 704. loss: 0.2277, acc: 93.27%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 705. loss: 0.2205, acc: 93.48%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 706. loss: 0.2270, acc: 93.52%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 707. loss: 0.2280, acc: 93.35%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 708. loss: 0.2238, acc: 93.51%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 709. loss: 0.2328, acc: 93.14%, val_loss 0.4898, val_acc 84.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 710. loss: 0.2287, acc: 93.22%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 711. loss: 0.2326, acc: 93.21%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 712. loss: 0.2297, acc: 92.92%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 713. loss: 0.2304, acc: 93.24%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 714. loss: 0.2317, acc: 93.05%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 715. loss: 0.2284, acc: 93.18%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 716. loss: 0.2257, acc: 93.16%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 717. loss: 0.2266, acc: 93.31%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 718. loss: 0.2307, acc: 93.00%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 719. loss: 0.2239, acc: 93.31%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 720. loss: 0.2271, acc: 93.37%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 721. loss: 0.2249, acc: 93.29%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 722. loss: 0.2270, acc: 93.28%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 723. loss: 0.2327, acc: 92.90%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 724. loss: 0.2316, acc: 93.01%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 725. loss: 0.2286, acc: 93.21%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 726. loss: 0.2284, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 727. loss: 0.2305, acc: 93.15%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 728. loss: 0.2341, acc: 93.27%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 729. loss: 0.2347, acc: 93.01%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 730. loss: 0.2276, acc: 93.01%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 731. loss: 0.2280, acc: 93.10%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 732. loss: 0.2299, acc: 92.99%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 733. loss: 0.2313, acc: 92.95%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 734. loss: 0.2359, acc: 93.00%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 735. loss: 0.2327, acc: 92.98%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 736. loss: 0.2291, acc: 93.04%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 737. loss: 0.2292, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 738. loss: 0.2359, acc: 92.82%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 739. loss: 0.2325, acc: 93.02%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 740. loss: 0.2311, acc: 93.30%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 741. loss: 0.2223, acc: 93.38%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 742. loss: 0.2296, acc: 93.17%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 743. loss: 0.2331, acc: 93.07%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 744. loss: 0.2345, acc: 93.14%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 745. loss: 0.2270, acc: 93.27%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 746. loss: 0.2269, acc: 93.14%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 747. loss: 0.2261, acc: 93.58%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 748. loss: 0.2348, acc: 93.03%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 749. loss: 0.2306, acc: 93.40%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 750. loss: 0.2318, acc: 93.14%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 751. loss: 0.2371, acc: 93.05%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 752. loss: 0.2299, acc: 93.28%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 753. loss: 0.2283, acc: 93.25%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 754. loss: 0.2282, acc: 93.26%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 755. loss: 0.2266, acc: 93.27%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 756. loss: 0.2272, acc: 93.56%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 757. loss: 0.2277, acc: 93.54%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 758. loss: 0.2307, acc: 93.14%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 759. loss: 0.2358, acc: 92.93%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 760. loss: 0.2337, acc: 92.98%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 761. loss: 0.2299, acc: 93.16%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 762. loss: 0.2275, acc: 93.35%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 763. loss: 0.2305, acc: 93.18%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 764. loss: 0.2294, acc: 93.24%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 765. loss: 0.2310, acc: 92.95%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 766. loss: 0.2298, acc: 92.99%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 767. loss: 0.2266, acc: 93.43%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 768. loss: 0.2306, acc: 93.15%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 769. loss: 0.2316, acc: 93.10%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 770. loss: 0.2260, acc: 93.46%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 771. loss: 0.2299, acc: 93.40%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 772. loss: 0.2251, acc: 93.19%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 773. loss: 0.2234, acc: 93.54%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 774. loss: 0.2204, acc: 93.61%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 775. loss: 0.2327, acc: 92.90%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 776. loss: 0.2232, acc: 93.22%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 777. loss: 0.2286, acc: 93.14%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 778. loss: 0.2264, acc: 92.94%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 779. loss: 0.2319, acc: 93.19%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 780. loss: 0.2261, acc: 93.41%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 781. loss: 0.2201, acc: 93.55%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 782. loss: 0.2263, acc: 93.37%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 783. loss: 0.2316, acc: 93.41%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 784. loss: 0.2342, acc: 93.29%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 785. loss: 0.2236, acc: 93.36%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 786. loss: 0.2282, acc: 93.49%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 787. loss: 0.2351, acc: 92.77%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 788. loss: 0.2343, acc: 92.94%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 789. loss: 0.2272, acc: 93.10%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 790. loss: 0.2302, acc: 93.41%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 791. loss: 0.2308, acc: 92.95%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 792. loss: 0.2241, acc: 93.48%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 793. loss: 0.2289, acc: 93.24%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 794. loss: 0.2303, acc: 93.15%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 795. loss: 0.2290, acc: 93.24%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 796. loss: 0.2271, acc: 93.35%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 797. loss: 0.2284, acc: 93.27%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 798. loss: 0.2306, acc: 93.04%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 799. loss: 0.2319, acc: 92.83%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 800. loss: 0.2272, acc: 93.36%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 801. loss: 0.2304, acc: 92.94%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 802. loss: 0.2275, acc: 93.22%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 803. loss: 0.2289, acc: 93.32%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 804. loss: 0.2311, acc: 93.21%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 805. loss: 0.2343, acc: 92.98%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 806. loss: 0.2260, acc: 93.50%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 807. loss: 0.2324, acc: 93.08%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 808. loss: 0.2249, acc: 93.47%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 809. loss: 0.2229, acc: 93.43%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 810. loss: 0.2337, acc: 93.04%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 811. loss: 0.2234, acc: 93.50%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 812. loss: 0.2324, acc: 93.05%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 813. loss: 0.2263, acc: 93.42%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 814. loss: 0.2247, acc: 93.15%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 815. loss: 0.2288, acc: 93.35%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 816. loss: 0.2290, acc: 93.13%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 817. loss: 0.2281, acc: 93.08%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 818. loss: 0.2301, acc: 93.02%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 819. loss: 0.2234, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 820. loss: 0.2331, acc: 92.79%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 821. loss: 0.2234, acc: 93.59%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 822. loss: 0.2254, acc: 93.34%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 823. loss: 0.2301, acc: 93.40%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 824. loss: 0.2239, acc: 93.18%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 825. loss: 0.2333, acc: 92.53%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 826. loss: 0.2225, acc: 93.44%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 827. loss: 0.2297, acc: 93.13%, val_loss 0.4898, val_acc 84.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 828. loss: 0.2290, acc: 93.31%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 829. loss: 0.2337, acc: 93.19%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 830. loss: 0.2249, acc: 93.58%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 831. loss: 0.2276, acc: 93.30%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 832. loss: 0.2363, acc: 92.98%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 833. loss: 0.2384, acc: 92.83%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 834. loss: 0.2283, acc: 93.22%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 835. loss: 0.2359, acc: 92.88%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 836. loss: 0.2296, acc: 93.19%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 837. loss: 0.2246, acc: 93.37%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 838. loss: 0.2294, acc: 93.27%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 839. loss: 0.2268, acc: 93.45%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 840. loss: 0.2206, acc: 93.21%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 841. loss: 0.2263, acc: 93.14%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 842. loss: 0.2316, acc: 93.27%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 843. loss: 0.2343, acc: 93.11%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 844. loss: 0.2308, acc: 93.48%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 845. loss: 0.2213, acc: 93.52%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 846. loss: 0.2247, acc: 93.53%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 847. loss: 0.2287, acc: 92.89%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 848. loss: 0.2300, acc: 93.24%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 849. loss: 0.2264, acc: 93.28%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 850. loss: 0.2287, acc: 93.02%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 851. loss: 0.2294, acc: 93.33%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 852. loss: 0.2208, acc: 93.58%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 853. loss: 0.2268, acc: 93.50%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 854. loss: 0.2354, acc: 93.07%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 855. loss: 0.2300, acc: 93.31%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 856. loss: 0.2318, acc: 93.14%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 857. loss: 0.2256, acc: 93.54%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 858. loss: 0.2353, acc: 92.85%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 859. loss: 0.2331, acc: 92.96%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 860. loss: 0.2292, acc: 93.26%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 861. loss: 0.2313, acc: 92.88%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 862. loss: 0.2306, acc: 93.12%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 863. loss: 0.2315, acc: 93.07%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 864. loss: 0.2298, acc: 93.21%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 865. loss: 0.2302, acc: 93.13%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 866. loss: 0.2243, acc: 93.37%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 867. loss: 0.2312, acc: 93.16%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 868. loss: 0.2344, acc: 92.76%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 869. loss: 0.2333, acc: 92.94%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 870. loss: 0.2293, acc: 93.25%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 871. loss: 0.2257, acc: 93.42%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 872. loss: 0.2393, acc: 92.75%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 873. loss: 0.2290, acc: 93.37%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 874. loss: 0.2327, acc: 93.00%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 875. loss: 0.2309, acc: 92.70%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 876. loss: 0.2270, acc: 93.30%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 877. loss: 0.2290, acc: 93.35%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 878. loss: 0.2260, acc: 93.22%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 879. loss: 0.2238, acc: 93.43%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 880. loss: 0.2338, acc: 92.88%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 881. loss: 0.2273, acc: 93.48%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 882. loss: 0.2272, acc: 93.21%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 883. loss: 0.2329, acc: 92.93%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 884. loss: 0.2290, acc: 93.54%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 885. loss: 0.2277, acc: 93.13%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 886. loss: 0.2361, acc: 92.58%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 887. loss: 0.2374, acc: 92.78%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 888. loss: 0.2291, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 889. loss: 0.2213, acc: 93.59%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 890. loss: 0.2282, acc: 93.12%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 891. loss: 0.2265, acc: 93.25%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 892. loss: 0.2289, acc: 92.88%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 893. loss: 0.2302, acc: 93.17%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 894. loss: 0.2305, acc: 93.23%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 895. loss: 0.2265, acc: 93.20%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 896. loss: 0.2281, acc: 93.26%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 897. loss: 0.2240, acc: 93.25%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 898. loss: 0.2342, acc: 93.03%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 899. loss: 0.2289, acc: 92.91%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 900. loss: 0.2293, acc: 93.35%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 901. loss: 0.2251, acc: 93.07%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 902. loss: 0.2304, acc: 93.30%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 903. loss: 0.2315, acc: 93.17%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 904. loss: 0.2288, acc: 93.22%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 905. loss: 0.2239, acc: 93.23%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 906. loss: 0.2224, acc: 93.51%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 907. loss: 0.2292, acc: 92.94%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 908. loss: 0.2292, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 909. loss: 0.2250, acc: 93.27%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 910. loss: 0.2302, acc: 92.99%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 911. loss: 0.2251, acc: 93.43%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 912. loss: 0.2308, acc: 93.14%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 913. loss: 0.2290, acc: 93.14%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 914. loss: 0.2254, acc: 93.30%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 915. loss: 0.2278, acc: 93.20%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 916. loss: 0.2369, acc: 92.61%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 917. loss: 0.2298, acc: 93.02%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 918. loss: 0.2243, acc: 93.33%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 919. loss: 0.2279, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 920. loss: 0.2326, acc: 92.94%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 921. loss: 0.2310, acc: 93.08%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 922. loss: 0.2288, acc: 93.10%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 923. loss: 0.2301, acc: 93.31%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 924. loss: 0.2301, acc: 93.23%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 925. loss: 0.2318, acc: 92.85%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 926. loss: 0.2296, acc: 93.21%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 927. loss: 0.2352, acc: 92.82%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 928. loss: 0.2323, acc: 93.17%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 929. loss: 0.2349, acc: 92.99%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 930. loss: 0.2350, acc: 92.94%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 931. loss: 0.2341, acc: 92.81%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 932. loss: 0.2280, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 933. loss: 0.2346, acc: 92.85%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 934. loss: 0.2256, acc: 93.23%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 935. loss: 0.2304, acc: 93.11%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 936. loss: 0.2359, acc: 93.00%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 937. loss: 0.2294, acc: 93.33%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 938. loss: 0.2259, acc: 93.33%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 939. loss: 0.2284, acc: 93.12%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 940. loss: 0.2302, acc: 93.34%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 941. loss: 0.2204, acc: 93.54%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 942. loss: 0.2366, acc: 92.74%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 943. loss: 0.2282, acc: 93.43%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 944. loss: 0.2272, acc: 93.33%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 945. loss: 0.2279, acc: 93.27%, val_loss 0.4898, val_acc 84.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 946. loss: 0.2329, acc: 92.73%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 947. loss: 0.2309, acc: 93.05%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 948. loss: 0.2270, acc: 93.34%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 949. loss: 0.2240, acc: 93.44%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 950. loss: 0.2228, acc: 93.72%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 951. loss: 0.2296, acc: 93.04%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 952. loss: 0.2351, acc: 92.94%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 953. loss: 0.2194, acc: 93.49%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 954. loss: 0.2268, acc: 93.26%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 955. loss: 0.2246, acc: 93.39%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 956. loss: 0.2267, acc: 93.24%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 957. loss: 0.2312, acc: 92.89%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 958. loss: 0.2331, acc: 93.06%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 959. loss: 0.2385, acc: 92.82%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 960. loss: 0.2222, acc: 93.35%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 961. loss: 0.2330, acc: 93.10%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 962. loss: 0.2288, acc: 92.99%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 963. loss: 0.2296, acc: 93.20%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 964. loss: 0.2339, acc: 92.95%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 965. loss: 0.2279, acc: 93.41%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 966. loss: 0.2299, acc: 93.16%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 967. loss: 0.2336, acc: 92.82%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 968. loss: 0.2261, acc: 93.42%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 969. loss: 0.2289, acc: 93.06%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 970. loss: 0.2260, acc: 93.15%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 971. loss: 0.2326, acc: 92.81%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 972. loss: 0.2369, acc: 92.77%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 973. loss: 0.2326, acc: 92.95%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 974. loss: 0.2250, acc: 93.17%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 975. loss: 0.2305, acc: 93.49%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 976. loss: 0.2271, acc: 93.66%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 977. loss: 0.2342, acc: 92.67%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 978. loss: 0.2248, acc: 93.35%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 979. loss: 0.2295, acc: 93.50%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 980. loss: 0.2366, acc: 93.03%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 981. loss: 0.2246, acc: 93.42%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 982. loss: 0.2303, acc: 93.35%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 983. loss: 0.2255, acc: 93.45%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 984. loss: 0.2295, acc: 93.00%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 985. loss: 0.2290, acc: 93.09%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 986. loss: 0.2319, acc: 93.04%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 987. loss: 0.2241, acc: 93.26%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 988. loss: 0.2241, acc: 93.43%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 989. loss: 0.2324, acc: 93.03%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 990. loss: 0.2330, acc: 92.99%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 991. loss: 0.2284, acc: 93.29%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 992. loss: 0.2217, acc: 93.45%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 993. loss: 0.2297, acc: 92.82%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 994. loss: 0.2352, acc: 92.92%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 995. loss: 0.2269, acc: 93.34%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 996. loss: 0.2287, acc: 93.28%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 997. loss: 0.2309, acc: 93.08%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 998. loss: 0.2262, acc: 93.33%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 999. loss: 0.2256, acc: 93.25%, val_loss 0.4898, val_acc 84.31%\n",
      "Epoch 1000. loss: 0.2364, acc: 93.15%, val_loss 0.4898, val_acc 84.31%\n"
     ]
    }
   ],
   "source": [
    "net = get_net(ctx)\n",
    "train_loss_list, val_loss_list = train(net, data_iter_train, data_iter_val, ctx, epochs=1000, lr=0.01, \\\n",
    "      mome=0.9, wd=1e-4, lr_decay=0.5, lr_period=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1.5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAVkDmargehYnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1233ef5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list, label=\"train_loss\")\n",
    "plt.plot(val_loss_list, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.ylim([0,1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_softmax = nd.softmax(net(nd.array(X_test_resnet50_v1).as_in_context(ctx)))\n",
    "\n",
    "synsets = mx.gluon.data.vision.ImageFolderDataset(\"/Users/shengwan/Desktop/data/train_gy\").synsets\n",
    "ids = sorted(os.listdir(\"/Users/shengwan/Desktop/data/test_gy/0/\"))\n",
    "ids = [i[:-4] for i in ids]\n",
    "\n",
    "df = pd.DataFrame(out_softmax.asnumpy())\n",
    "df.columns = synsets\n",
    "df[\"id\"] = ids\n",
    "df = df[[\"id\"]+synsets]\n",
    "df.to_csv('/Users/shengwan/Desktop/data/pred-resnet50-v1.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
