{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import mxnet as mx\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/shengwan/Desktop/data\"\n",
    "train_dir = \"train\"\n",
    "test_dir = \"test\"\n",
    "train_gy_dir = \"train_gy\"\n",
    "test_gy_dir = \"test_gy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove original directory\n",
    "train_gy_path = os.path.join(data_dir, train_gy_dir)\n",
    "if os.path.exists(train_gy_path):\n",
    "    shutil.rmtree(train_gy_path)\n",
    "    \n",
    "# make new directory\n",
    "if not os.path.exists(train_gy_path):\n",
    "    os.makedirs(train_gy_path)\n",
    "    \n",
    "# get training data id and labels\n",
    "id_labels = pd.read_csv(os.path.join(data_dir, \"labels.csv\"))\n",
    "\n",
    "# construct sym link between train_dir and train_gy_dir\n",
    "for _, (curr_id, curr_breed) in id_labels.iterrows():\n",
    "    dst_dir = os.path.join(train_gy_path, curr_breed)\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.makedirs(dst_dir)\n",
    "    src_loc = os.path.join(data_dir, train_dir, curr_id+\".jpg\")\n",
    "    dst_loc = os.path.join(dst_dir, curr_id+\".jpg\")\n",
    "    os.symlink(src_loc, dst_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove original directory\n",
    "test_gy_path = os.path.join(data_dir, test_gy_dir)\n",
    "if os.path.exists(test_gy_path):\n",
    "    shutil.rmtree(test_gy_path)\n",
    "    \n",
    "# make new directory\n",
    "if not os.path.exists(test_gy_path):\n",
    "    os.makedirs(test_gy_path)\n",
    "\n",
    "# construct sym link between test_dir and test_gy_dir\n",
    "for roor_dir, sub_dir, sub_files in os.walk(os.path.join(data_dir, test_dir)):\n",
    "    for sub_file in sub_files:\n",
    "        dst_dir = os.path.join(test_gy_path, \"0\")\n",
    "        if not os.path.exists(dst_dir):\n",
    "            os.makedirs(dst_dir)\n",
    "        src_loc = os.path.join(data_dir, test_dir, sub_file)\n",
    "        dst_loc = os.path.join(dst_dir, sub_file)\n",
    "        os.symlink(src_loc, dst_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resize images for model\n",
    "preprocess_list = [\n",
    "    lambda img: img.astype(\"float32\")/255,\n",
    "    mx.image.ForceResizeAug((224, 224)),\n",
    "    mx.image.ColorNormalizeAug(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n",
    "    lambda img: nd.transpose(img,(2,0,1))\n",
    "]\n",
    "\n",
    "def image_preprocess(img):\n",
    "    for f in preprocess_list:\n",
    "        img = f(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "def transform(img, label):\n",
    "    return image_preprocess(img), label\n",
    "\n",
    "def load_data(data_dir, load_batch_size = 32, f_trans=transform):\n",
    "    imgs = mx.gluon.data.vision.ImageFolderDataset(data_dir, transform=transform)\n",
    "    data = mx.gluon.data.DataLoader(imgs, load_batch_size, last_batch=\"keep\")\n",
    "    return data\n",
    "\n",
    "#Extract features\n",
    "def extract_features(net, data, ctx):\n",
    "    rst_X, rst_y = [], []\n",
    "    for X, y in tqdm(data):\n",
    "        Xi = net.features(X.as_in_context(ctx))\n",
    "        rst_X.append(Xi.asnumpy())\n",
    "        rst_y.append(y.asnumpy())\n",
    "    return np.concatenate(rst_X, axis=0), np.concatenate(rst_y, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(\"/Users/shengwan/Desktop/data/train_gy\")\n",
    "test_data = load_data(\"/Users/shengwan/Desktop/data/test_gy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file is not found. Downloading.\n",
      "Downloading /Users/shengwan/.mxnet/models/resnet152_v2-64c75ac8.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/resnet152_v2-64c75ac8.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [39:04<00:00,  7.33s/it]\n",
      "100%|██████████| 324/324 [39:51<00:00,  7.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# Extract features\n",
    "ctx = mx.cpu()\n",
    "resnet152_v2 = mx.gluon.model_zoo.vision.resnet152_v2(pretrained=True, ctx=ctx)\n",
    "X_train_resnet152_v2, y_train = extract_features(resnet152_v2, train_data, ctx)\n",
    "X_test_resnet152_v2, _ = extract_features(resnet152_v2, test_data, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save features\n",
    "import h5py\n",
    "with h5py.File('/Users/shengwan/Desktop/data/resnet152_v2_pretrained_Xy.h5', 'w') as f:\n",
    "    f['X_train_resnet152_v2'] = X_train_resnet152_v2\n",
    "    f['X_test_resnet152_v2'] = X_test_resnet152_v2\n",
    "    f['y_train'] = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "with h5py.File('/Users/shengwan/Desktop/data/resnet152_v2_pretrained_Xy.h5', 'r') as f:\n",
    "    X_train_resnet152_v2 = np.array(f['X_train_resnet152_v2'])\n",
    "    X_test_resnet152_v2_v2 = np.array(f['X_test_resnet152_v2'])\n",
    "    y_train = np.array(f['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatch\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_resnet152_v2, y_train, test_size=0.2)\n",
    "\n",
    "# dataset\n",
    "dataset_train = mx.gluon.data.ArrayDataset(nd.array(X_train), nd.array(y_train))\n",
    "dataset_val = mx.gluon.data.ArrayDataset(nd.array(X_val), nd.array(y_val))\n",
    "\n",
    "# data itet\n",
    "batch_size = 128\n",
    "data_iter_train = mx.gluon.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "data_iter_val = mx.gluon.data.DataLoader(dataset_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    return nd.mean(nd.argmax(output, axis=1) == labels).asscalar()\n",
    "\n",
    "def evaluate(net, data_iter):\n",
    "    softmax_cross_entropy = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    loss, acc, n = 0., 0., 0.\n",
    "    steps = len(data_iter)\n",
    "    for data, label in data_iter:\n",
    "        data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        acc += accuracy(output, label)\n",
    "        loss += nd.mean(softmax_cross_entropy(output, label)).asscalar()\n",
    "    return loss/steps, acc/steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "ctx = mx.cpu()\n",
    "def get_net(ctx):\n",
    "\n",
    "    net = nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(nn.Dense(256, activation='relu'))\n",
    "        net.add(nn.Dropout(0.5))\n",
    "        net.add(nn.Dense(120))\n",
    "\n",
    "    net.initialize(ctx=ctx)\n",
    "    return net\n",
    "\n",
    "#train\n",
    "def train(net, data_iter_train, data_iter_val, ctx, \n",
    "          epochs=50, lr=0.01, mome=0.9, wd=1e-4, lr_decay=0.5, lr_period=20):\n",
    "\n",
    "    softmax_cross_entropy = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = mx.gluon.Trainer(net.collect_params(),  'sgd', {'learning_rate': lr, 'momentum': mome, \n",
    "                                      'wd': wd})\n",
    "    \n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        steps = len(data_iter_train)\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for X, y in data_iter_train:\n",
    "\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "\n",
    "            with mx.autograd.record():\n",
    "                out = net(X)\n",
    "                loss = softmax_cross_entropy(out, y)\n",
    "\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "\n",
    "\n",
    "            train_acc += accuracy(out, y)\n",
    "\n",
    "        val_loss, val_acc = evaluate(net, data_iter_val)\n",
    "        train_loss_list.append(train_loss/steps)\n",
    "        val_loss_list.append(val_loss)\n",
    "        print(\"Epoch %d. loss: %.4f, acc: %.2f%%, val_loss %.4f, val_acc %.2f%%\" % (\n",
    "            epoch+1, train_loss/steps, train_acc/steps*100, val_loss, val_acc*100))\n",
    "        \n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. loss: 4.0054, acc: 17.97%, val_loss 2.3848, val_acc 63.91%\n",
      "Epoch 2. loss: 1.7197, acc: 60.16%, val_loss 0.7954, val_acc 86.16%\n",
      "Epoch 3. loss: 0.9432, acc: 75.48%, val_loss 0.5374, val_acc 86.70%\n",
      "Epoch 4. loss: 0.7216, acc: 79.73%, val_loss 0.4391, val_acc 88.51%\n",
      "Epoch 5. loss: 0.6294, acc: 81.48%, val_loss 0.4124, val_acc 88.22%\n",
      "Epoch 6. loss: 0.5489, acc: 83.97%, val_loss 0.3844, val_acc 88.51%\n",
      "Epoch 7. loss: 0.5189, acc: 84.53%, val_loss 0.3639, val_acc 88.85%\n",
      "Epoch 8. loss: 0.4871, acc: 85.80%, val_loss 0.3475, val_acc 89.44%\n",
      "Epoch 9. loss: 0.4461, acc: 85.99%, val_loss 0.3438, val_acc 88.66%\n",
      "Epoch 10. loss: 0.4258, acc: 86.64%, val_loss 0.3349, val_acc 89.29%\n",
      "Epoch 11. loss: 0.4007, acc: 87.75%, val_loss 0.3318, val_acc 89.49%\n",
      "Epoch 12. loss: 0.3985, acc: 87.16%, val_loss 0.3291, val_acc 90.03%\n",
      "Epoch 13. loss: 0.3771, acc: 88.20%, val_loss 0.3209, val_acc 89.29%\n",
      "Epoch 14. loss: 0.3687, acc: 88.37%, val_loss 0.3144, val_acc 89.68%\n",
      "Epoch 15. loss: 0.3513, acc: 88.84%, val_loss 0.3234, val_acc 89.39%\n",
      "Epoch 16. loss: 0.3419, acc: 89.48%, val_loss 0.3151, val_acc 89.64%\n",
      "Epoch 17. loss: 0.3323, acc: 89.66%, val_loss 0.3219, val_acc 89.98%\n",
      "Epoch 18. loss: 0.3268, acc: 90.03%, val_loss 0.3196, val_acc 89.05%\n",
      "Epoch 19. loss: 0.3144, acc: 90.14%, val_loss 0.3116, val_acc 89.49%\n",
      "Epoch 20. loss: 0.3046, acc: 90.33%, val_loss 0.3135, val_acc 89.88%\n",
      "Epoch 21. loss: 0.2912, acc: 90.50%, val_loss 0.3073, val_acc 89.64%\n",
      "Epoch 22. loss: 0.2806, acc: 91.27%, val_loss 0.3100, val_acc 89.73%\n",
      "Epoch 23. loss: 0.2825, acc: 91.46%, val_loss 0.3040, val_acc 90.32%\n",
      "Epoch 24. loss: 0.2727, acc: 91.60%, val_loss 0.3064, val_acc 89.83%\n",
      "Epoch 25. loss: 0.2653, acc: 91.56%, val_loss 0.3033, val_acc 90.22%\n",
      "Epoch 26. loss: 0.2608, acc: 91.84%, val_loss 0.3073, val_acc 89.54%\n",
      "Epoch 27. loss: 0.2575, acc: 91.90%, val_loss 0.3107, val_acc 89.88%\n",
      "Epoch 28. loss: 0.2567, acc: 91.81%, val_loss 0.3045, val_acc 89.83%\n",
      "Epoch 29. loss: 0.2570, acc: 91.87%, val_loss 0.3059, val_acc 89.63%\n",
      "Epoch 30. loss: 0.2551, acc: 91.89%, val_loss 0.3055, val_acc 90.08%\n",
      "Epoch 31. loss: 0.2500, acc: 91.79%, val_loss 0.3070, val_acc 90.03%\n",
      "Epoch 32. loss: 0.2400, acc: 92.54%, val_loss 0.3038, val_acc 89.93%\n",
      "Epoch 33. loss: 0.2441, acc: 92.19%, val_loss 0.3060, val_acc 89.93%\n",
      "Epoch 34. loss: 0.2306, acc: 92.92%, val_loss 0.3039, val_acc 89.78%\n",
      "Epoch 35. loss: 0.2423, acc: 92.29%, val_loss 0.3061, val_acc 90.03%\n",
      "Epoch 36. loss: 0.2359, acc: 92.59%, val_loss 0.3051, val_acc 90.03%\n",
      "Epoch 37. loss: 0.2351, acc: 92.62%, val_loss 0.3064, val_acc 90.03%\n",
      "Epoch 38. loss: 0.2292, acc: 92.84%, val_loss 0.3033, val_acc 90.03%\n",
      "Epoch 39. loss: 0.2282, acc: 92.70%, val_loss 0.3019, val_acc 90.32%\n",
      "Epoch 40. loss: 0.2220, acc: 92.95%, val_loss 0.3046, val_acc 89.93%\n",
      "Epoch 41. loss: 0.2179, acc: 93.21%, val_loss 0.3013, val_acc 89.88%\n",
      "Epoch 42. loss: 0.2152, acc: 93.46%, val_loss 0.3002, val_acc 90.03%\n",
      "Epoch 43. loss: 0.2197, acc: 93.05%, val_loss 0.3032, val_acc 90.17%\n",
      "Epoch 44. loss: 0.2134, acc: 93.22%, val_loss 0.3032, val_acc 89.88%\n",
      "Epoch 45. loss: 0.2164, acc: 93.45%, val_loss 0.3018, val_acc 90.03%\n",
      "Epoch 46. loss: 0.2139, acc: 93.33%, val_loss 0.3009, val_acc 90.17%\n",
      "Epoch 47. loss: 0.2080, acc: 93.80%, val_loss 0.3042, val_acc 90.03%\n",
      "Epoch 48. loss: 0.2133, acc: 93.21%, val_loss 0.3039, val_acc 90.03%\n",
      "Epoch 49. loss: 0.2065, acc: 93.28%, val_loss 0.3046, val_acc 89.83%\n",
      "Epoch 50. loss: 0.2085, acc: 93.75%, val_loss 0.3028, val_acc 89.83%\n",
      "Epoch 51. loss: 0.2058, acc: 93.72%, val_loss 0.3036, val_acc 90.12%\n",
      "Epoch 52. loss: 0.2021, acc: 93.77%, val_loss 0.3058, val_acc 90.03%\n",
      "Epoch 53. loss: 0.2007, acc: 93.75%, val_loss 0.3052, val_acc 89.93%\n",
      "Epoch 54. loss: 0.1988, acc: 93.80%, val_loss 0.3023, val_acc 89.78%\n",
      "Epoch 55. loss: 0.2000, acc: 94.09%, val_loss 0.3032, val_acc 90.27%\n",
      "Epoch 56. loss: 0.2033, acc: 93.70%, val_loss 0.3024, val_acc 89.83%\n",
      "Epoch 57. loss: 0.1994, acc: 93.45%, val_loss 0.3022, val_acc 90.12%\n",
      "Epoch 58. loss: 0.1988, acc: 93.61%, val_loss 0.3037, val_acc 89.93%\n",
      "Epoch 59. loss: 0.1971, acc: 93.62%, val_loss 0.3052, val_acc 89.98%\n",
      "Epoch 60. loss: 0.1948, acc: 93.64%, val_loss 0.3041, val_acc 90.12%\n",
      "Epoch 61. loss: 0.1925, acc: 94.18%, val_loss 0.3036, val_acc 89.93%\n",
      "Epoch 62. loss: 0.1936, acc: 94.06%, val_loss 0.3028, val_acc 89.68%\n",
      "Epoch 63. loss: 0.1938, acc: 94.07%, val_loss 0.3018, val_acc 90.07%\n",
      "Epoch 64. loss: 0.1926, acc: 93.95%, val_loss 0.3024, val_acc 90.22%\n",
      "Epoch 65. loss: 0.1917, acc: 94.49%, val_loss 0.3024, val_acc 90.02%\n",
      "Epoch 66. loss: 0.1893, acc: 94.40%, val_loss 0.3027, val_acc 89.98%\n",
      "Epoch 67. loss: 0.1856, acc: 94.40%, val_loss 0.3028, val_acc 89.98%\n",
      "Epoch 68. loss: 0.1890, acc: 94.25%, val_loss 0.3021, val_acc 90.07%\n",
      "Epoch 69. loss: 0.1915, acc: 93.85%, val_loss 0.3008, val_acc 90.08%\n",
      "Epoch 70. loss: 0.1885, acc: 94.10%, val_loss 0.3031, val_acc 89.98%\n",
      "Epoch 71. loss: 0.1881, acc: 94.40%, val_loss 0.3032, val_acc 90.17%\n",
      "Epoch 72. loss: 0.1844, acc: 94.25%, val_loss 0.3039, val_acc 90.07%\n",
      "Epoch 73. loss: 0.1899, acc: 94.16%, val_loss 0.3043, val_acc 89.88%\n",
      "Epoch 74. loss: 0.1858, acc: 94.27%, val_loss 0.3034, val_acc 89.93%\n",
      "Epoch 75. loss: 0.1875, acc: 94.48%, val_loss 0.3030, val_acc 90.07%\n",
      "Epoch 76. loss: 0.1847, acc: 94.41%, val_loss 0.3028, val_acc 90.12%\n",
      "Epoch 77. loss: 0.1858, acc: 94.27%, val_loss 0.3024, val_acc 90.17%\n",
      "Epoch 78. loss: 0.1905, acc: 93.96%, val_loss 0.3030, val_acc 89.78%\n",
      "Epoch 79. loss: 0.1788, acc: 94.50%, val_loss 0.3031, val_acc 89.88%\n",
      "Epoch 80. loss: 0.1837, acc: 94.20%, val_loss 0.3041, val_acc 89.93%\n",
      "Epoch 81. loss: 0.1837, acc: 94.39%, val_loss 0.3043, val_acc 89.88%\n",
      "Epoch 82. loss: 0.1853, acc: 94.23%, val_loss 0.3032, val_acc 89.88%\n",
      "Epoch 83. loss: 0.1818, acc: 94.32%, val_loss 0.3033, val_acc 89.98%\n",
      "Epoch 84. loss: 0.1820, acc: 94.47%, val_loss 0.3034, val_acc 90.02%\n",
      "Epoch 85. loss: 0.1822, acc: 94.69%, val_loss 0.3028, val_acc 90.07%\n",
      "Epoch 86. loss: 0.1751, acc: 94.78%, val_loss 0.3033, val_acc 89.93%\n",
      "Epoch 87. loss: 0.1801, acc: 94.48%, val_loss 0.3022, val_acc 89.93%\n",
      "Epoch 88. loss: 0.1776, acc: 94.39%, val_loss 0.3028, val_acc 90.17%\n",
      "Epoch 89. loss: 0.1787, acc: 94.36%, val_loss 0.3025, val_acc 90.22%\n",
      "Epoch 90. loss: 0.1850, acc: 94.40%, val_loss 0.3036, val_acc 90.22%\n",
      "Epoch 91. loss: 0.1815, acc: 94.58%, val_loss 0.3034, val_acc 90.17%\n",
      "Epoch 92. loss: 0.1715, acc: 94.72%, val_loss 0.3037, val_acc 90.03%\n",
      "Epoch 93. loss: 0.1835, acc: 94.34%, val_loss 0.3038, val_acc 90.08%\n",
      "Epoch 94. loss: 0.1793, acc: 94.44%, val_loss 0.3037, val_acc 90.08%\n",
      "Epoch 95. loss: 0.1790, acc: 94.33%, val_loss 0.3035, val_acc 89.83%\n",
      "Epoch 96. loss: 0.1786, acc: 94.50%, val_loss 0.3033, val_acc 90.12%\n",
      "Epoch 97. loss: 0.1782, acc: 94.50%, val_loss 0.3037, val_acc 90.08%\n",
      "Epoch 98. loss: 0.1739, acc: 94.83%, val_loss 0.3040, val_acc 89.98%\n",
      "Epoch 99. loss: 0.1721, acc: 94.90%, val_loss 0.3039, val_acc 89.98%\n",
      "Epoch 100. loss: 0.1740, acc: 94.84%, val_loss 0.3040, val_acc 90.07%\n",
      "Epoch 101. loss: 0.1769, acc: 94.58%, val_loss 0.3041, val_acc 89.88%\n",
      "Epoch 102. loss: 0.1753, acc: 94.77%, val_loss 0.3033, val_acc 89.93%\n",
      "Epoch 103. loss: 0.1736, acc: 94.85%, val_loss 0.3032, val_acc 89.98%\n",
      "Epoch 104. loss: 0.1791, acc: 94.61%, val_loss 0.3033, val_acc 90.03%\n",
      "Epoch 105. loss: 0.1719, acc: 94.71%, val_loss 0.3033, val_acc 90.02%\n",
      "Epoch 106. loss: 0.1734, acc: 94.68%, val_loss 0.3038, val_acc 89.98%\n",
      "Epoch 107. loss: 0.1785, acc: 94.57%, val_loss 0.3039, val_acc 90.02%\n",
      "Epoch 108. loss: 0.1777, acc: 94.71%, val_loss 0.3037, val_acc 89.93%\n",
      "Epoch 109. loss: 0.1731, acc: 94.93%, val_loss 0.3035, val_acc 89.88%\n",
      "Epoch 110. loss: 0.1799, acc: 94.51%, val_loss 0.3035, val_acc 89.88%\n",
      "Epoch 111. loss: 0.1788, acc: 94.62%, val_loss 0.3039, val_acc 89.88%\n",
      "Epoch 112. loss: 0.1762, acc: 94.51%, val_loss 0.3040, val_acc 89.98%\n",
      "Epoch 113. loss: 0.1758, acc: 94.74%, val_loss 0.3038, val_acc 89.83%\n",
      "Epoch 114. loss: 0.1723, acc: 94.66%, val_loss 0.3036, val_acc 89.88%\n",
      "Epoch 115. loss: 0.1746, acc: 94.67%, val_loss 0.3031, val_acc 89.83%\n",
      "Epoch 116. loss: 0.1792, acc: 94.55%, val_loss 0.3034, val_acc 89.98%\n",
      "Epoch 117. loss: 0.1712, acc: 95.01%, val_loss 0.3038, val_acc 90.03%\n",
      "Epoch 118. loss: 0.1815, acc: 94.15%, val_loss 0.3037, val_acc 89.98%\n",
      "Epoch 119. loss: 0.1683, acc: 95.09%, val_loss 0.3038, val_acc 90.08%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120. loss: 0.1740, acc: 94.76%, val_loss 0.3041, val_acc 90.03%\n",
      "Epoch 121. loss: 0.1729, acc: 94.73%, val_loss 0.3040, val_acc 90.08%\n",
      "Epoch 122. loss: 0.1749, acc: 94.50%, val_loss 0.3041, val_acc 90.08%\n",
      "Epoch 123. loss: 0.1782, acc: 94.79%, val_loss 0.3041, val_acc 90.08%\n",
      "Epoch 124. loss: 0.1725, acc: 95.04%, val_loss 0.3041, val_acc 90.03%\n",
      "Epoch 125. loss: 0.1717, acc: 94.71%, val_loss 0.3040, val_acc 90.03%\n",
      "Epoch 126. loss: 0.1705, acc: 95.24%, val_loss 0.3040, val_acc 90.08%\n",
      "Epoch 127. loss: 0.1731, acc: 94.77%, val_loss 0.3040, val_acc 89.98%\n",
      "Epoch 128. loss: 0.1693, acc: 94.96%, val_loss 0.3042, val_acc 89.98%\n",
      "Epoch 129. loss: 0.1780, acc: 94.79%, val_loss 0.3040, val_acc 89.93%\n",
      "Epoch 130. loss: 0.1752, acc: 94.70%, val_loss 0.3039, val_acc 89.88%\n",
      "Epoch 131. loss: 0.1799, acc: 94.50%, val_loss 0.3038, val_acc 89.98%\n",
      "Epoch 132. loss: 0.1734, acc: 94.69%, val_loss 0.3038, val_acc 89.98%\n",
      "Epoch 133. loss: 0.1764, acc: 94.69%, val_loss 0.3038, val_acc 89.93%\n",
      "Epoch 134. loss: 0.1732, acc: 94.89%, val_loss 0.3037, val_acc 89.98%\n",
      "Epoch 135. loss: 0.1797, acc: 94.40%, val_loss 0.3038, val_acc 89.98%\n",
      "Epoch 136. loss: 0.1737, acc: 94.76%, val_loss 0.3037, val_acc 89.93%\n",
      "Epoch 137. loss: 0.1741, acc: 94.57%, val_loss 0.3037, val_acc 89.93%\n",
      "Epoch 138. loss: 0.1766, acc: 94.42%, val_loss 0.3038, val_acc 90.12%\n",
      "Epoch 139. loss: 0.1723, acc: 94.78%, val_loss 0.3038, val_acc 90.12%\n",
      "Epoch 140. loss: 0.1791, acc: 94.44%, val_loss 0.3038, val_acc 90.03%\n",
      "Epoch 141. loss: 0.1742, acc: 94.91%, val_loss 0.3038, val_acc 90.08%\n",
      "Epoch 142. loss: 0.1780, acc: 94.23%, val_loss 0.3037, val_acc 90.08%\n",
      "Epoch 143. loss: 0.1751, acc: 94.76%, val_loss 0.3038, val_acc 90.12%\n",
      "Epoch 144. loss: 0.1721, acc: 95.09%, val_loss 0.3039, val_acc 90.08%\n",
      "Epoch 145. loss: 0.1681, acc: 94.89%, val_loss 0.3039, val_acc 90.03%\n",
      "Epoch 146. loss: 0.1770, acc: 94.54%, val_loss 0.3039, val_acc 90.08%\n",
      "Epoch 147. loss: 0.1698, acc: 94.89%, val_loss 0.3038, val_acc 90.12%\n",
      "Epoch 148. loss: 0.1629, acc: 95.10%, val_loss 0.3038, val_acc 90.08%\n",
      "Epoch 149. loss: 0.1748, acc: 94.78%, val_loss 0.3038, val_acc 90.17%\n",
      "Epoch 150. loss: 0.1667, acc: 95.07%, val_loss 0.3038, val_acc 90.03%\n",
      "Epoch 151. loss: 0.1742, acc: 94.82%, val_loss 0.3038, val_acc 90.03%\n",
      "Epoch 152. loss: 0.1743, acc: 94.83%, val_loss 0.3038, val_acc 90.03%\n",
      "Epoch 153. loss: 0.1760, acc: 94.76%, val_loss 0.3037, val_acc 89.98%\n",
      "Epoch 154. loss: 0.1712, acc: 94.98%, val_loss 0.3037, val_acc 89.93%\n",
      "Epoch 155. loss: 0.1703, acc: 95.01%, val_loss 0.3038, val_acc 89.93%\n",
      "Epoch 156. loss: 0.1748, acc: 94.79%, val_loss 0.3038, val_acc 90.08%\n",
      "Epoch 157. loss: 0.1720, acc: 94.70%, val_loss 0.3036, val_acc 90.12%\n",
      "Epoch 158. loss: 0.1715, acc: 94.81%, val_loss 0.3037, val_acc 90.03%\n",
      "Epoch 159. loss: 0.1740, acc: 94.64%, val_loss 0.3036, val_acc 90.12%\n",
      "Epoch 160. loss: 0.1719, acc: 94.74%, val_loss 0.3036, val_acc 90.03%\n",
      "Epoch 161. loss: 0.1737, acc: 94.82%, val_loss 0.3036, val_acc 90.03%\n",
      "Epoch 162. loss: 0.1736, acc: 94.77%, val_loss 0.3036, val_acc 90.08%\n",
      "Epoch 163. loss: 0.1677, acc: 94.90%, val_loss 0.3036, val_acc 90.03%\n",
      "Epoch 164. loss: 0.1669, acc: 95.14%, val_loss 0.3036, val_acc 90.03%\n",
      "Epoch 165. loss: 0.1710, acc: 94.81%, val_loss 0.3036, val_acc 90.03%\n",
      "Epoch 166. loss: 0.1673, acc: 94.96%, val_loss 0.3036, val_acc 90.03%\n",
      "Epoch 167. loss: 0.1717, acc: 94.99%, val_loss 0.3036, val_acc 90.03%\n",
      "Epoch 168. loss: 0.1743, acc: 94.81%, val_loss 0.3036, val_acc 90.03%\n",
      "Epoch 169. loss: 0.1799, acc: 94.63%, val_loss 0.3036, val_acc 89.98%\n",
      "Epoch 170. loss: 0.1729, acc: 94.79%, val_loss 0.3036, val_acc 90.08%\n",
      "Epoch 171. loss: 0.1777, acc: 94.67%, val_loss 0.3036, val_acc 89.93%\n",
      "Epoch 172. loss: 0.1656, acc: 95.26%, val_loss 0.3036, val_acc 89.98%\n",
      "Epoch 173. loss: 0.1746, acc: 94.75%, val_loss 0.3036, val_acc 89.93%\n",
      "Epoch 174. loss: 0.1711, acc: 94.88%, val_loss 0.3036, val_acc 89.93%\n",
      "Epoch 175. loss: 0.1726, acc: 94.70%, val_loss 0.3037, val_acc 89.93%\n",
      "Epoch 176. loss: 0.1708, acc: 94.87%, val_loss 0.3037, val_acc 90.03%\n",
      "Epoch 177. loss: 0.1729, acc: 94.54%, val_loss 0.3037, val_acc 89.98%\n",
      "Epoch 178. loss: 0.1724, acc: 94.89%, val_loss 0.3037, val_acc 89.93%\n",
      "Epoch 179. loss: 0.1687, acc: 94.97%, val_loss 0.3037, val_acc 89.98%\n",
      "Epoch 180. loss: 0.1665, acc: 95.19%, val_loss 0.3037, val_acc 89.93%\n",
      "Epoch 181. loss: 0.1707, acc: 94.72%, val_loss 0.3037, val_acc 89.93%\n",
      "Epoch 182. loss: 0.1676, acc: 94.95%, val_loss 0.3037, val_acc 89.93%\n",
      "Epoch 183. loss: 0.1734, acc: 94.51%, val_loss 0.3037, val_acc 89.98%\n",
      "Epoch 184. loss: 0.1726, acc: 94.78%, val_loss 0.3037, val_acc 89.98%\n",
      "Epoch 185. loss: 0.1761, acc: 94.93%, val_loss 0.3037, val_acc 89.98%\n",
      "Epoch 186. loss: 0.1733, acc: 94.83%, val_loss 0.3037, val_acc 89.98%\n",
      "Epoch 187. loss: 0.1714, acc: 94.89%, val_loss 0.3037, val_acc 89.98%\n",
      "Epoch 188. loss: 0.1670, acc: 95.14%, val_loss 0.3037, val_acc 89.98%\n",
      "Epoch 189. loss: 0.1673, acc: 94.90%, val_loss 0.3037, val_acc 89.98%\n",
      "Epoch 190. loss: 0.1739, acc: 94.65%, val_loss 0.3037, val_acc 89.98%\n",
      "Epoch 191. loss: 0.1770, acc: 94.80%, val_loss 0.3037, val_acc 89.98%\n",
      "Epoch 192. loss: 0.1722, acc: 94.52%, val_loss 0.3037, val_acc 89.93%\n",
      "Epoch 193. loss: 0.1711, acc: 94.80%, val_loss 0.3037, val_acc 89.93%\n",
      "Epoch 194. loss: 0.1765, acc: 94.63%, val_loss 0.3037, val_acc 89.98%\n",
      "Epoch 195. loss: 0.1733, acc: 94.69%, val_loss 0.3037, val_acc 89.93%\n",
      "Epoch 196. loss: 0.1702, acc: 94.73%, val_loss 0.3037, val_acc 89.93%\n",
      "Epoch 197. loss: 0.1738, acc: 94.65%, val_loss 0.3037, val_acc 89.93%\n",
      "Epoch 198. loss: 0.1674, acc: 95.08%, val_loss 0.3037, val_acc 89.93%\n",
      "Epoch 199. loss: 0.1741, acc: 94.74%, val_loss 0.3037, val_acc 89.93%\n",
      "Epoch 200. loss: 0.1768, acc: 94.54%, val_loss 0.3037, val_acc 89.88%\n"
     ]
    }
   ],
   "source": [
    "net = get_net(ctx)\n",
    "train_loss_list, val_loss_list = train(net, data_iter_train, data_iter_val, ctx, epochs=200, lr=0.01, \\\n",
    "      mome=0.9, wd=1e-4, lr_decay=0.5, lr_period=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1.5)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1263ce2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list, label=\"train_loss\")\n",
    "plt.plot(val_loss_list, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.ylim([0,1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_softmax = nd.softmax(net(nd.array(X_test_resnet152_v2).as_in_context(ctx)))\n",
    "\n",
    "synsets = mx.gluon.data.vision.ImageFolderDataset(\"/Users/shengwan/Desktop/data/train_gy\").synsets\n",
    "ids = sorted(os.listdir(\"/Users/shengwan/Desktop/data/test_gy/0/\"))\n",
    "ids = [i[:-4] for i in ids]\n",
    "\n",
    "df = pd.DataFrame(out_softmax.asnumpy())\n",
    "df.columns = synsets\n",
    "df[\"id\"] = ids\n",
    "df = df[[\"id\"]+synsets]\n",
    "df.to_csv('/Users/shengwan/Desktop/data/pred-resnet152_v2.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
